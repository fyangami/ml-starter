{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    with open('./the_time_machine.txt', 'r') as txt:\n",
    "        lines = txt.readlines()\n",
    "    import re\n",
    "    return [\n",
    "        l for l in\n",
    "        [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "        if l.strip() != ''\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3093"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    return [list(line) if token == 'char' else line.split() for line in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        counter = Vocab.count_corpus(tokens)\n",
    "        # 对词频率排序\n",
    "        self.__token_freqs = sorted(counter.items(),\n",
    "                                    key=lambda x: x[1],\n",
    "                                    reverse=True)\n",
    "        self.index_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_index = {\n",
    "            token: idx\n",
    "            for idx, token in enumerate(self.index_to_token)\n",
    "        }\n",
    "        for token, freq in self.__token_freqs:\n",
    "            if freq >= min_freq and token not in self.token_to_index:\n",
    "                self.index_to_token.append(token)\n",
    "                self.token_to_index[token] = len(self.index_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_to_token)\n",
    "\n",
    "    def get_tokens(self, indicates):\n",
    "        if not isinstance(indicates, (list, tuple)):\n",
    "            return self.index_to_token[indicates]\n",
    "        return ''.join([self.get_tokens(index) for index in indicates])\n",
    "    \n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_index.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self.__token_freqs\n",
    "\n",
    "    @staticmethod\n",
    "    def count_corpus(tokens):\n",
    "        if isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        from collections import Counter\n",
    "        return Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(lines)\n",
    "vocab = Vocab(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2477),\n",
       " ('and', 1312),\n",
       " ('of', 1286),\n",
       " ('i', 1268),\n",
       " ('a', 877),\n",
       " ('to', 766),\n",
       " ('in', 606),\n",
       " ('was', 554),\n",
       " ('that', 458),\n",
       " ('it', 452),\n",
       " ('my', 441),\n",
       " ('had', 354),\n",
       " ('as', 281),\n",
       " ('me', 281),\n",
       " ('with', 264),\n",
       " ('at', 257),\n",
       " ('for', 247),\n",
       " ('you', 212),\n",
       " ('time', 211),\n",
       " ('but', 209),\n",
       " ('this', 199),\n",
       " ('or', 162),\n",
       " ('were', 158),\n",
       " ('on', 148),\n",
       " ('not', 142),\n",
       " ('from', 137),\n",
       " ('all', 136),\n",
       " ('then', 134),\n",
       " ('is', 129),\n",
       " ('have', 129),\n",
       " ('his', 129),\n",
       " ('there', 128),\n",
       " ('by', 126),\n",
       " ('he', 126),\n",
       " ('they', 124),\n",
       " ('one', 120),\n",
       " ('upon', 115),\n",
       " ('so', 114),\n",
       " ('into', 114),\n",
       " ('little', 114),\n",
       " ('be', 112),\n",
       " ('came', 107),\n",
       " ('no', 102),\n",
       " ('gutenberg', 98),\n",
       " ('some', 95),\n",
       " ('machine', 93),\n",
       " ('could', 93),\n",
       " ('an', 92),\n",
       " ('which', 92),\n",
       " ('we', 91),\n",
       " ('their', 91),\n",
       " ('said', 89),\n",
       " ('project', 88),\n",
       " ('saw', 88),\n",
       " ('down', 87),\n",
       " ('s', 86),\n",
       " ('very', 86),\n",
       " ('them', 86),\n",
       " ('now', 79),\n",
       " ('what', 78),\n",
       " ('these', 77),\n",
       " ('about', 77),\n",
       " ('any', 75),\n",
       " ('been', 75),\n",
       " ('her', 75),\n",
       " ('up', 74),\n",
       " ('out', 74),\n",
       " ('like', 74),\n",
       " ('if', 72),\n",
       " ('its', 72),\n",
       " ('seemed', 72),\n",
       " ('are', 71),\n",
       " ('man', 71),\n",
       " ('thing', 66),\n",
       " ('traveller', 65),\n",
       " ('again', 62),\n",
       " ('white', 61),\n",
       " ('our', 61),\n",
       " ('more', 60),\n",
       " ('would', 60),\n",
       " ('must', 59),\n",
       " ('when', 58),\n",
       " ('thought', 57),\n",
       " ('felt', 57),\n",
       " ('tm', 57),\n",
       " ('work', 55),\n",
       " ('weena', 54),\n",
       " ('other', 53),\n",
       " ('even', 53),\n",
       " ('still', 53),\n",
       " ('before', 52),\n",
       " ('over', 52),\n",
       " ('through', 51),\n",
       " ('myself', 51),\n",
       " ('people', 49),\n",
       " ('hand', 49),\n",
       " ('went', 49),\n",
       " ('first', 49),\n",
       " ('may', 48),\n",
       " ('morlocks', 48),\n",
       " ('only', 48),\n",
       " ('see', 48),\n",
       " ('last', 47),\n",
       " ('how', 47),\n",
       " ('towards', 47),\n",
       " ('found', 47),\n",
       " ('will', 46),\n",
       " ('she', 46),\n",
       " ('too', 45),\n",
       " ('here', 43),\n",
       " ('light', 43),\n",
       " ('great', 42),\n",
       " ('did', 41),\n",
       " ('away', 40),\n",
       " ('him', 40),\n",
       " ('way', 40),\n",
       " ('began', 40),\n",
       " ('back', 40),\n",
       " ('world', 39),\n",
       " ('under', 39),\n",
       " ('after', 39),\n",
       " ('do', 39),\n",
       " ('night', 38),\n",
       " ('face', 38),\n",
       " ('such', 38),\n",
       " ('same', 38),\n",
       " ('than', 37),\n",
       " ('another', 37),\n",
       " ('well', 37),\n",
       " ('where', 36),\n",
       " ('us', 36),\n",
       " ('things', 36),\n",
       " ('think', 36),\n",
       " ('round', 36),\n",
       " ('made', 36),\n",
       " ('eyes', 35),\n",
       " ('mind', 35),\n",
       " ('long', 35),\n",
       " ('might', 35),\n",
       " ('perhaps', 35),\n",
       " ('put', 34),\n",
       " ('looked', 34),\n",
       " ('own', 34),\n",
       " ('most', 33),\n",
       " ('against', 33),\n",
       " ('among', 33),\n",
       " ('sky', 33),\n",
       " ('can', 32),\n",
       " ('took', 32),\n",
       " ('day', 32),\n",
       " ('strange', 32),\n",
       " ('yet', 32),\n",
       " ('works', 32),\n",
       " ('new', 31),\n",
       " ('moment', 31),\n",
       " ('old', 31),\n",
       " ('come', 31),\n",
       " ('sun', 31),\n",
       " ('fire', 30),\n",
       " ('know', 30),\n",
       " ('black', 30),\n",
       " ('darkness', 29),\n",
       " ('who', 29),\n",
       " ('green', 28),\n",
       " ('two', 28),\n",
       " ('t', 28),\n",
       " ('enough', 28),\n",
       " ('off', 28),\n",
       " ('hands', 28),\n",
       " ('presently', 28),\n",
       " ('place', 27),\n",
       " ('dark', 27),\n",
       " ('once', 27),\n",
       " ('electronic', 27),\n",
       " ('red', 26),\n",
       " ('end', 26),\n",
       " ('left', 26),\n",
       " ('grew', 26),\n",
       " ('full', 26),\n",
       " ('almost', 25),\n",
       " ('psychologist', 25),\n",
       " ('three', 25),\n",
       " ('space', 25),\n",
       " ('got', 25),\n",
       " ('should', 25),\n",
       " ('part', 25),\n",
       " ('foundation', 25),\n",
       " ('terms', 24),\n",
       " ('sphinx', 24),\n",
       " ('much', 24),\n",
       " ('has', 24),\n",
       " ('looking', 24),\n",
       " ('medical', 24),\n",
       " ('future', 24),\n",
       " ('stood', 24),\n",
       " ('fear', 24),\n",
       " ('above', 23),\n",
       " ('make', 23),\n",
       " ('air', 23),\n",
       " ('head', 23),\n",
       " ('e', 23),\n",
       " ('set', 22),\n",
       " ('sat', 22),\n",
       " ('without', 22),\n",
       " ('tried', 22),\n",
       " ('cannot', 22),\n",
       " ('far', 22),\n",
       " ('seen', 22),\n",
       " ('minute', 22),\n",
       " ('suddenly', 22),\n",
       " ('across', 22),\n",
       " ('soon', 21),\n",
       " ('along', 21),\n",
       " ('kind', 21),\n",
       " ('get', 21),\n",
       " ('earth', 21),\n",
       " ('turned', 21),\n",
       " ('while', 21),\n",
       " ('human', 21),\n",
       " ('use', 20),\n",
       " ('indeed', 20),\n",
       " ('heard', 20),\n",
       " ('say', 20),\n",
       " ('gone', 20),\n",
       " ('never', 20),\n",
       " ('until', 20),\n",
       " ('look', 20),\n",
       " ('certain', 20),\n",
       " ('already', 20),\n",
       " ('half', 20),\n",
       " ('editor', 20),\n",
       " ('feeling', 20),\n",
       " ('hill', 20),\n",
       " ('copyright', 20),\n",
       " ('states', 19),\n",
       " ('though', 19),\n",
       " ('go', 19),\n",
       " ('room', 19),\n",
       " ('laboratory', 19),\n",
       " ('those', 19),\n",
       " ('bronze', 19),\n",
       " ('flowers', 19),\n",
       " ('match', 19),\n",
       " ('matches', 19),\n",
       " ('gallery', 19),\n",
       " ('license', 18),\n",
       " ('rather', 18),\n",
       " ('right', 18),\n",
       " ('years', 18),\n",
       " ('feet', 18),\n",
       " ('moon', 18),\n",
       " ('agreement', 18),\n",
       " ('palace', 17),\n",
       " ('story', 17),\n",
       " ('filby', 17),\n",
       " ('ground', 17),\n",
       " ('however', 17),\n",
       " ('just', 17),\n",
       " ('your', 17),\n",
       " ('table', 17),\n",
       " ('behind', 17),\n",
       " ('lever', 17),\n",
       " ('big', 17),\n",
       " ('life', 17),\n",
       " ('ran', 17),\n",
       " ('door', 17),\n",
       " ('days', 17),\n",
       " ('united', 16),\n",
       " ('age', 16),\n",
       " ('soft', 16),\n",
       " ('mere', 16),\n",
       " ('don', 16),\n",
       " ('between', 16),\n",
       " ('each', 16),\n",
       " ('why', 16),\n",
       " ('fell', 16),\n",
       " ('morning', 16),\n",
       " ('something', 16),\n",
       " ('going', 16),\n",
       " ('past', 16),\n",
       " ('followed', 16),\n",
       " ('lay', 16),\n",
       " ('less', 16),\n",
       " ('lit', 16),\n",
       " ('four', 15),\n",
       " ('dimensions', 15),\n",
       " ('side', 15),\n",
       " ('whole', 15),\n",
       " ('small', 15),\n",
       " ('bright', 15),\n",
       " ('knew', 15),\n",
       " ('struck', 15),\n",
       " ('running', 15),\n",
       " ('creatures', 15),\n",
       " ('find', 15),\n",
       " ('wood', 15),\n",
       " ('donations', 15),\n",
       " ('travelling', 14),\n",
       " ('being', 14),\n",
       " ('large', 14),\n",
       " ('men', 14),\n",
       " ('odd', 14),\n",
       " ('understand', 14),\n",
       " ('good', 14),\n",
       " ('told', 14),\n",
       " ('forth', 14),\n",
       " ('next', 14),\n",
       " ('feel', 14),\n",
       " ('ever', 14),\n",
       " ('bushes', 14),\n",
       " ('coming', 14),\n",
       " ('literary', 14),\n",
       " ('trademark', 14),\n",
       " ('ebook', 13),\n",
       " ('sudden', 13),\n",
       " ('passed', 13),\n",
       " ('dinner', 13),\n",
       " ('course', 13),\n",
       " ('clearly', 13),\n",
       " ('lamp', 13),\n",
       " ('clear', 13),\n",
       " ('others', 13),\n",
       " ('rose', 13),\n",
       " ('move', 13),\n",
       " ('tell', 13),\n",
       " ('second', 13),\n",
       " ('silent', 13),\n",
       " ('times', 13),\n",
       " ('hundred', 13),\n",
       " ('doubt', 13),\n",
       " ('suppose', 13),\n",
       " ('dust', 13),\n",
       " ('nothing', 13),\n",
       " ('every', 13),\n",
       " ('dim', 13),\n",
       " ('open', 13),\n",
       " ('huge', 13),\n",
       " ('hall', 13),\n",
       " ('sea', 13),\n",
       " ('archive', 13),\n",
       " ('copy', 12),\n",
       " ('porcelain', 12),\n",
       " ('because', 12),\n",
       " ('am', 12),\n",
       " ('several', 12),\n",
       " ('vanished', 12),\n",
       " ('since', 12),\n",
       " ('below', 12),\n",
       " ('remember', 12),\n",
       " ('gave', 12),\n",
       " ('sound', 12),\n",
       " ('happened', 12),\n",
       " ('trees', 12),\n",
       " ('creature', 12),\n",
       " ('vast', 12),\n",
       " ('beautiful', 12),\n",
       " ('pocket', 12),\n",
       " ('nature', 12),\n",
       " ('within', 12),\n",
       " ('wells', 11),\n",
       " ('further', 11),\n",
       " ('pale', 11),\n",
       " ('grey', 11),\n",
       " ('line', 11),\n",
       " ('really', 11),\n",
       " ('except', 11),\n",
       " ('simply', 11),\n",
       " ('figure', 11),\n",
       " ('state', 11),\n",
       " ('eight', 11),\n",
       " ('done', 11),\n",
       " ('save', 11),\n",
       " ('possibly', 11),\n",
       " ('absolutely', 11),\n",
       " ('conditions', 11),\n",
       " ('sense', 11),\n",
       " ('queer', 11),\n",
       " ('evening', 11),\n",
       " ('either', 11),\n",
       " ('hesitated', 11),\n",
       " ('glass', 11),\n",
       " ('stars', 11),\n",
       " ('growing', 11),\n",
       " ('pedestal', 11),\n",
       " ('appeared', 11),\n",
       " ('thick', 11),\n",
       " ('altogether', 11),\n",
       " ('judged', 11),\n",
       " ('few', 11),\n",
       " ('blackness', 11),\n",
       " ('camphor', 11),\n",
       " ('slower', 11),\n",
       " ('paragraph', 11),\n",
       " ('f', 11),\n",
       " ('www', 10),\n",
       " ('org', 10),\n",
       " ('laws', 10),\n",
       " ('caught', 10),\n",
       " ('instance', 10),\n",
       " ('wrong', 10),\n",
       " ('idea', 10),\n",
       " ('always', 10),\n",
       " ('quite', 10),\n",
       " ('exactly', 10),\n",
       " ('animal', 10),\n",
       " ('better', 10),\n",
       " ('let', 10),\n",
       " ('walked', 10),\n",
       " ('larger', 10),\n",
       " ('m', 10),\n",
       " ('believe', 10),\n",
       " ('travelled', 10),\n",
       " ('bars', 10),\n",
       " ('met', 10),\n",
       " ('watch', 10),\n",
       " ('stopped', 10),\n",
       " ('sleep', 10),\n",
       " ('peculiar', 10),\n",
       " ('house', 10),\n",
       " ('humanity', 10),\n",
       " ('lawn', 10),\n",
       " ('creeping', 10),\n",
       " ('imagine', 10),\n",
       " ('memory', 10),\n",
       " ('number', 10),\n",
       " ('broken', 10),\n",
       " ('many', 10),\n",
       " ('strong', 10),\n",
       " ('determined', 10),\n",
       " ('physical', 10),\n",
       " ('alone', 10),\n",
       " ('ruins', 10),\n",
       " ('comfort', 10),\n",
       " ('including', 10),\n",
       " ('underworld', 10),\n",
       " ('arms', 10),\n",
       " ('iron', 10),\n",
       " ('access', 10),\n",
       " ('refund', 10),\n",
       " ('sunset', 9),\n",
       " ('explanation', 9),\n",
       " ('return', 9),\n",
       " ('free', 9),\n",
       " ('follow', 9),\n",
       " ('shall', 9),\n",
       " ('real', 9),\n",
       " ('nor', 9),\n",
       " ('explain', 9),\n",
       " ('ago', 9),\n",
       " ('moving', 9),\n",
       " ('travel', 9),\n",
       " ('means', 9),\n",
       " ('interest', 9),\n",
       " ('arm', 9),\n",
       " ('shoulder', 9),\n",
       " ('bar', 9),\n",
       " ('laughed', 9),\n",
       " ('stared', 9),\n",
       " ('nearly', 9),\n",
       " ('moved', 9),\n",
       " ('point', 9),\n",
       " ('plain', 9),\n",
       " ('none', 9),\n",
       " ('standing', 9),\n",
       " ('journalist', 9),\n",
       " ('opened', 9),\n",
       " ('brown', 9),\n",
       " ('faint', 9),\n",
       " ('question', 9),\n",
       " ('change', 9),\n",
       " ('rest', 9),\n",
       " ('apparently', 9),\n",
       " ('blue', 9),\n",
       " ('buildings', 9),\n",
       " ('hung', 9),\n",
       " ('stone', 9),\n",
       " ('lost', 9),\n",
       " ('fancied', 9),\n",
       " ('intellectual', 9),\n",
       " ('entered', 9),\n",
       " ('near', 9),\n",
       " ('necessity', 9),\n",
       " ('form', 9),\n",
       " ('strength', 9),\n",
       " ('social', 9),\n",
       " ('perfect', 9),\n",
       " ('doors', 9),\n",
       " ('associated', 9),\n",
       " ('section', 9),\n",
       " ('agree', 9),\n",
       " ('give', 8),\n",
       " ('shone', 8),\n",
       " ('need', 8),\n",
       " ('length', 8),\n",
       " ('became', 8),\n",
       " ('fact', 8),\n",
       " ('young', 8),\n",
       " ('surface', 8),\n",
       " ('telling', 8),\n",
       " ('different', 8),\n",
       " ('forward', 8),\n",
       " ('hope', 8),\n",
       " ('model', 8),\n",
       " ('also', 8),\n",
       " ('together', 8),\n",
       " ('motion', 8),\n",
       " ('faster', 8),\n",
       " ('itself', 8),\n",
       " ('beside', 8),\n",
       " ('take', 8),\n",
       " ('eye', 8),\n",
       " ('wanted', 8),\n",
       " ('colour', 8),\n",
       " ('cut', 8),\n",
       " ('faces', 8),\n",
       " ('meat', 8),\n",
       " ('thinking', 8),\n",
       " ('noticed', 8),\n",
       " ('assured', 8),\n",
       " ('horrible', 8),\n",
       " ('swiftly', 8),\n",
       " ('unknown', 8),\n",
       " ('everything', 8),\n",
       " ('drove', 8),\n",
       " ('instead', 8),\n",
       " ('race', 8),\n",
       " ('remote', 8),\n",
       " ('figures', 8),\n",
       " ('pretty', 8),\n",
       " ('thousand', 8),\n",
       " ('children', 8),\n",
       " ('loose', 8),\n",
       " ('received', 8),\n",
       " ('building', 8),\n",
       " ('general', 8),\n",
       " ('floor', 8),\n",
       " ('least', 8),\n",
       " ('view', 8),\n",
       " ('abundant', 8),\n",
       " ('security', 8),\n",
       " ('needs', 8),\n",
       " ('triumph', 8),\n",
       " ('decay', 8),\n",
       " ('cold', 8),\n",
       " ('sleeping', 8),\n",
       " ('daylight', 8),\n",
       " ('taken', 8),\n",
       " ('hastily', 8),\n",
       " ('glare', 8),\n",
       " ('shaft', 8),\n",
       " ('eloi', 8),\n",
       " ('south', 8),\n",
       " ('box', 8),\n",
       " ('forest', 8),\n",
       " ('law', 8),\n",
       " ('fee', 8),\n",
       " ('information', 8),\n",
       " ('anyone', 7),\n",
       " ('located', 7),\n",
       " ('country', 7),\n",
       " ('golden', 7),\n",
       " ('accepted', 7),\n",
       " ('mean', 7),\n",
       " ('anything', 7),\n",
       " ('natural', 7),\n",
       " ('dimension', 7),\n",
       " ('trace', 7),\n",
       " ('high', 7),\n",
       " ('certainly', 7),\n",
       " ('hard', 7),\n",
       " ('sure', 7),\n",
       " ('freely', 7),\n",
       " ('present', 7),\n",
       " ('become', 7),\n",
       " ('stop', 7),\n",
       " ('account', 7),\n",
       " ('attention', 7),\n",
       " ('case', 7),\n",
       " ('leave', 7),\n",
       " ('slowly', 7),\n",
       " ('held', 7),\n",
       " ('mechanism', 7),\n",
       " ('want', 7),\n",
       " ('saddle', 7),\n",
       " ('spoke', 7),\n",
       " ('laughing', 7),\n",
       " ('sounds', 7),\n",
       " ('shadows', 7),\n",
       " ('puzzled', 7),\n",
       " ('somehow', 7),\n",
       " ('paper', 7),\n",
       " ('comes', 7),\n",
       " ('mouth', 7),\n",
       " ('intense', 7),\n",
       " ('pushed', 7),\n",
       " ('seem', 7),\n",
       " ('hear', 7),\n",
       " ('rare', 7),\n",
       " ('kept', 7),\n",
       " ('rising', 7),\n",
       " ('worn', 7),\n",
       " ('shadow', 7),\n",
       " ('short', 7),\n",
       " ('ears', 7),\n",
       " ('fast', 7),\n",
       " ('splendid', 7),\n",
       " ('shivered', 7),\n",
       " ('longer', 7),\n",
       " ('absolute', 7),\n",
       " ('beyond', 7),\n",
       " ('tree', 7),\n",
       " ('touched', 7),\n",
       " ('breathing', 7),\n",
       " ('violently', 7),\n",
       " ('following', 7),\n",
       " ('fancy', 7),\n",
       " ('levers', 7),\n",
       " ('neck', 7),\n",
       " ('foot', 7),\n",
       " ('windows', 7),\n",
       " ('metal', 7),\n",
       " ('seated', 7),\n",
       " ('fruit', 7),\n",
       " ('tired', 7),\n",
       " ('river', 7),\n",
       " ('valley', 7),\n",
       " ('help', 7),\n",
       " ('living', 7),\n",
       " ('during', 7),\n",
       " ('intelligence', 7),\n",
       " ('grow', 7),\n",
       " ('pleasant', 7),\n",
       " ('problem', 7),\n",
       " ('trying', 7),\n",
       " ('narrow', 7),\n",
       " ('water', 7),\n",
       " ('protected', 7),\n",
       " ('beach', 7),\n",
       " ('returned', 7),\n",
       " ('fallen', 7),\n",
       " ('underground', 7),\n",
       " ('thousands', 7),\n",
       " ('motionless', 7),\n",
       " ('dream', 7),\n",
       " ('u', 7),\n",
       " ('distributing', 7),\n",
       " ('copies', 7),\n",
       " ('ebooks', 7),\n",
       " ('provide', 7),\n",
       " ('parts', 6),\n",
       " ('using', 6),\n",
       " ('date', 6),\n",
       " ('language', 6),\n",
       " ('start', 6),\n",
       " ('mankind', 6),\n",
       " ('speak', 6),\n",
       " ('matter', 6),\n",
       " ('silver', 6),\n",
       " ('carefully', 6),\n",
       " ('person', 6),\n",
       " ('hair', 6),\n",
       " ('existence', 6),\n",
       " ('body', 6),\n",
       " ('wait', 6),\n",
       " ('does', 6),\n",
       " ('fourth', 6),\n",
       " ('direction', 6),\n",
       " ('beginning', 6),\n",
       " ('making', 6),\n",
       " ('efforts', 6),\n",
       " ('hold', 6),\n",
       " ('words', 6),\n",
       " ('evidently', 6),\n",
       " ('getting', 6),\n",
       " ('reason', 6),\n",
       " ('vague', 6),\n",
       " ('suggested', 6),\n",
       " ('cried', 6),\n",
       " ('trick', 6),\n",
       " ('ivory', 6),\n",
       " ('unless', 6),\n",
       " ('front', 6),\n",
       " ('drew', 6),\n",
       " ('chair', 6),\n",
       " ('incredible', 6),\n",
       " ('pointed', 6),\n",
       " ('wind', 6),\n",
       " ('bare', 6),\n",
       " ('impression', 6),\n",
       " ('flickering', 6),\n",
       " ('dance', 6),\n",
       " ('showed', 6),\n",
       " ('confusion', 6),\n",
       " ('late', 6),\n",
       " ('seven', 6),\n",
       " ('quiet', 6),\n",
       " ('word', 6),\n",
       " ('warm', 6),\n",
       " ('blood', 6),\n",
       " ('read', 6),\n",
       " ('waiting', 6),\n",
       " ('hot', 6),\n",
       " ('home', 6),\n",
       " ('both', 6),\n",
       " ('started', 6),\n",
       " ('reached', 6),\n",
       " ('smoking', 6),\n",
       " ('lived', 6),\n",
       " ('thud', 6),\n",
       " ('garden', 6),\n",
       " ('afraid', 6),\n",
       " ('convey', 6),\n",
       " ('unpleasant', 6),\n",
       " ('fall', 6),\n",
       " ('twilight', 6),\n",
       " ('hillside', 6),\n",
       " ('contact', 6),\n",
       " ('possible', 6),\n",
       " ('turf', 6),\n",
       " ('smoke', 6),\n",
       " ('nearer', 6),\n",
       " ('straight', 6),\n",
       " ('reminded', 6),\n",
       " ('used', 6),\n",
       " ('sight', 6),\n",
       " ('confidence', 6),\n",
       " ('danger', 6),\n",
       " ('hitherto', 6),\n",
       " ('effort', 6),\n",
       " ('import', 6),\n",
       " ('corner', 6),\n",
       " ('watching', 6),\n",
       " ('slow', 6),\n",
       " ('discovered', 6),\n",
       " ('distance', 6),\n",
       " ('slope', 6),\n",
       " ('plants', 6),\n",
       " ('disappeared', 6),\n",
       " ('close', 6),\n",
       " ('west', 6),\n",
       " ('signs', 6),\n",
       " ('truth', 6),\n",
       " ('animals', 6),\n",
       " ('limited', 6),\n",
       " ('fate', 6),\n",
       " ('north', 6),\n",
       " ('sometimes', 6),\n",
       " ('slept', 6),\n",
       " ('panels', 6),\n",
       " ('stir', 6),\n",
       " ('mystery', 6),\n",
       " ('presence', 6),\n",
       " ('machinery', 6),\n",
       " ('theory', 6),\n",
       " ('heat', 6),\n",
       " ('species', 6),\n",
       " ('familiar', 6),\n",
       " ('distribute', 6),\n",
       " ('permission', 6),\n",
       " ('charge', 6),\n",
       " ('distribution', 6),\n",
       " ('comply', 6),\n",
       " ('paid', 6),\n",
       " ('tax', 6),\n",
       " ('convenient', 5),\n",
       " ('burnt', 5),\n",
       " ('paradox', 5),\n",
       " ('thickness', 5),\n",
       " ('having', 5),\n",
       " ('object', 5),\n",
       " ('tendency', 5),\n",
       " ('slight', 5),\n",
       " ('difference', 5),\n",
       " ('foolish', 5),\n",
       " ('particularly', 5),\n",
       " ('lips', 5),\n",
       " ('pause', 5),\n",
       " ('gently', 5),\n",
       " ('recognised', 5),\n",
       " ('smiled', 5),\n",
       " ('velocity', 5),\n",
       " ('himself', 5),\n",
       " ('wild', 5),\n",
       " ('deep', 5),\n",
       " ('wonder', 5),\n",
       " ('metallic', 5),\n",
       " ('clock', 5),\n",
       " ('tables', 5),\n",
       " ('brass', 5),\n",
       " ('low', 5),\n",
       " ('watched', 5),\n",
       " ('notice', 5),\n",
       " ('pressed', 5),\n",
       " ('pass', 5),\n",
       " ('waste', 5),\n",
       " ('changed', 5),\n",
       " ('individual', 5),\n",
       " ('breath', 5),\n",
       " ('flame', 5),\n",
       " ('ghost', 5),\n",
       " ('pipe', 5),\n",
       " ('interval', 5),\n",
       " ('thursday', 5),\n",
       " ('simple', 5),\n",
       " ('common', 5),\n",
       " ('led', 5),\n",
       " ('corridor', 5),\n",
       " ('complete', 5),\n",
       " ('perfectly', 5),\n",
       " ('holding', 5),\n",
       " ('perceived', 5),\n",
       " ('considerable', 5),\n",
       " ('ll', 5),\n",
       " ('previous', 5),\n",
       " ('spirit', 5),\n",
       " ('surprise', 5),\n",
       " ('coat', 5),\n",
       " ('doorway', 5),\n",
       " ('brighter', 5),\n",
       " ('till', 5),\n",
       " ('business', 5),\n",
       " ('easy', 5),\n",
       " ('remained', 5),\n",
       " ('startled', 5),\n",
       " ('salt', 5),\n",
       " ('displayed', 5),\n",
       " ('true', 5),\n",
       " ('writing', 5),\n",
       " ('ceased', 5),\n",
       " ('ten', 5),\n",
       " ('falling', 5),\n",
       " ('hazy', 5),\n",
       " ('sensations', 5),\n",
       " ('helpless', 5),\n",
       " ('suggestion', 5),\n",
       " ('early', 5),\n",
       " ('fair', 5),\n",
       " ('possession', 5),\n",
       " ('civilisation', 5),\n",
       " ('occurred', 5),\n",
       " ('resolved', 5),\n",
       " ('incontinently', 5),\n",
       " ('hail', 5),\n",
       " ('shape', 5),\n",
       " ('carried', 5),\n",
       " ('hour', 5),\n",
       " ('inhuman', 5),\n",
       " ('distinct', 5),\n",
       " ('wall', 5),\n",
       " ('clad', 5),\n",
       " ('rich', 5),\n",
       " ('voices', 5),\n",
       " ('heads', 5),\n",
       " ('beauty', 5),\n",
       " ('sweet', 5),\n",
       " ('ease', 5),\n",
       " ('flinging', 5),\n",
       " ('forgotten', 5),\n",
       " ('staggered', 5),\n",
       " ('knowledge', 5),\n",
       " ('naturally', 5),\n",
       " ('deserted', 5),\n",
       " ('mass', 5),\n",
       " ('generations', 5),\n",
       " ('ways', 5),\n",
       " ('heaps', 5),\n",
       " ('fruits', 5),\n",
       " ('lower', 5),\n",
       " ('nevertheless', 5),\n",
       " ('spite', 5),\n",
       " ('cattle', 5),\n",
       " ('particular', 5),\n",
       " ('later', 5),\n",
       " ('learn', 5),\n",
       " ('name', 5),\n",
       " ('cries', 5),\n",
       " ('calm', 5),\n",
       " ('crest', 5),\n",
       " ('planet', 5),\n",
       " ('masses', 5),\n",
       " ('experience', 5),\n",
       " ('secure', 5),\n",
       " ('yellow', 5),\n",
       " ('horizon', 5),\n",
       " ('steadily', 5),\n",
       " ('leaving', 5),\n",
       " ('current', 5),\n",
       " ('hither', 5),\n",
       " ('thither', 5),\n",
       " ('increasing', 5),\n",
       " ('support', 5),\n",
       " ('restless', 5),\n",
       " ('energy', 5),\n",
       " ('weak', 5),\n",
       " ('pain', 5),\n",
       " ('folly', 5),\n",
       " ('removed', 5),\n",
       " ('covered', 5),\n",
       " ('whose', 5),\n",
       " ('circumstances', 5),\n",
       " ('poor', 5),\n",
       " ('probably', 5),\n",
       " ('keep', 5),\n",
       " ('horror', 5),\n",
       " ('land', 5),\n",
       " ('path', 5),\n",
       " ('reflection', 5),\n",
       " ('safe', 5),\n",
       " ('afternoon', 5),\n",
       " ('best', 5),\n",
       " ('carry', 5),\n",
       " ('dreaded', 5),\n",
       " ('nights', 5),\n",
       " ('clambering', 5),\n",
       " ('outside', 5),\n",
       " ('fingers', 5),\n",
       " ('edge', 5),\n",
       " ('habit', 5),\n",
       " ('museum', 5),\n",
       " ('burning', 5),\n",
       " ('killing', 5),\n",
       " ('eastward', 5),\n",
       " ('grass', 5),\n",
       " ('provided', 5),\n",
       " ('phrase', 5),\n",
       " ('compliance', 5),\n",
       " ('donate', 5),\n",
       " ('medium', 5),\n",
       " ('replacement', 5),\n",
       " ('volunteers', 5),\n",
       " ('online', 4),\n",
       " ('check', 4),\n",
       " ('character', 4),\n",
       " ('trap', 4),\n",
       " ('brightly', 4),\n",
       " ('flashed', 4),\n",
       " ('geometry', 4),\n",
       " ('expect', 4),\n",
       " ('begin', 4),\n",
       " ('admit', 4),\n",
       " ('proceeded', 4),\n",
       " ('call', 4),\n",
       " ('latter', 4),\n",
       " ('cigar', 4),\n",
       " ('provincial', 4),\n",
       " ('mayor', 4),\n",
       " ('manner', 4),\n",
       " ('curious', 4),\n",
       " ('proper', 4),\n",
       " ('weather', 4),\n",
       " ('movement', 4),\n",
       " ('freedom', 4),\n",
       " ('passing', 4),\n",
       " ('miles', 4),\n",
       " ('difficulty', 4),\n",
       " ('savage', 4),\n",
       " ('show', 4),\n",
       " ('laughter', 4),\n",
       " ('money', 4),\n",
       " ('discover', 4),\n",
       " ('weary', 4),\n",
       " ('smiling', 4),\n",
       " ('passage', 4),\n",
       " ('scarcely', 4),\n",
       " ('substance', 4),\n",
       " ('legs', 4),\n",
       " ('singularly', 4),\n",
       " ('seat', 4),\n",
       " ('turning', 4),\n",
       " ('blown', 4),\n",
       " ('indistinct', 4),\n",
       " ('damned', 4),\n",
       " ('journey', 4),\n",
       " ('visible', 4),\n",
       " ('serious', 4),\n",
       " ('remarked', 4),\n",
       " ('asked', 4),\n",
       " ('plausible', 4),\n",
       " ('tomorrow', 4),\n",
       " ('broad', 4),\n",
       " ('bench', 4),\n",
       " ('sheets', 4),\n",
       " ('touch', 4),\n",
       " ('easily', 4),\n",
       " ('whom', 4),\n",
       " ('five', 4),\n",
       " ('d', 4),\n",
       " ('ve', 4),\n",
       " ('besides', 4),\n",
       " ('absence', 4),\n",
       " ('week', 4),\n",
       " ('facing', 4),\n",
       " ('smile', 4),\n",
       " ('remembered', 4),\n",
       " ('brought', 4),\n",
       " ('doing', 4),\n",
       " ('recover', 4),\n",
       " ('resumed', 4),\n",
       " ('curiosity', 4),\n",
       " ('friend', 4),\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['that',\n",
       "  'i',\n",
       "  'noticed',\n",
       "  'for',\n",
       "  'the',\n",
       "  'first',\n",
       "  'time',\n",
       "  'how',\n",
       "  'warm',\n",
       "  'the',\n",
       "  'air',\n",
       "  'was'],\n",
       " [9, 4, 518, 17, 1, 98, 19, 104, 698, 1, 199, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[666], vocab[tokens[666]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def load_corpus():\n",
    "    lines = read_data()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    return corpus, vocab\n",
    "\n",
    "\n",
    "def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用随机抽样生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1\n",
    "    import random\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    # 减去1，是因为我们需要考虑标签\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    # 长度为num_steps的子序列的起始索引\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # 在随机抽样的迭代过程中，\n",
    "    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # 返回从pos位置开始的长度为num_steps的序列\n",
    "        return corpus[pos:pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # 在这里，initial_indices包含子序列的随机起始索引\n",
    "        initial_indices_per_batch = initial_indices[i:i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield tf.constant(X), tf.constant(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, vocab = load_corpus()\n",
    "BATCH_SIZE = 32\n",
    "NUM_STEPS = 35\n",
    "train_iter = seq_data_iter_random(corpus=corpus,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  num_steps=NUM_STEPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1\\200 loss: 3.190975\n",
      "epoch[2\\200 loss: 2.975364\n",
      "epoch[3\\200 loss: 2.937062\n",
      "epoch[4\\200 loss: 2.949771\n",
      "epoch[5\\200 loss: 2.939920\n",
      "epoch[6\\200 loss: 2.886942\n",
      "epoch[7\\200 loss: 2.872908\n",
      "epoch[8\\200 loss: 2.878511\n",
      "epoch[9\\200 loss: 2.858157\n",
      "epoch[10\\200 loss: 2.866439\n",
      "epoch[11\\200 loss: 2.836256\n",
      "epoch[12\\200 loss: 2.859394\n",
      "epoch[13\\200 loss: 2.846761\n",
      "epoch[14\\200 loss: 2.825873\n",
      "epoch[15\\200 loss: 2.844437\n",
      "epoch[16\\200 loss: 2.808188\n",
      "epoch[17\\200 loss: 2.812666\n",
      "epoch[18\\200 loss: 2.813853\n",
      "epoch[19\\200 loss: 2.848081\n",
      "epoch[20\\200 loss: 2.788270\n",
      "epoch[21\\200 loss: 2.790801\n",
      "epoch[22\\200 loss: 2.804196\n",
      "epoch[23\\200 loss: 2.778377\n",
      "epoch[24\\200 loss: 2.799523\n",
      "epoch[25\\200 loss: 2.803141\n",
      "epoch[26\\200 loss: 2.781680\n",
      "epoch[27\\200 loss: 2.747699\n",
      "epoch[28\\200 loss: 2.774217\n",
      "epoch[29\\200 loss: 2.752084\n",
      "epoch[30\\200 loss: 2.784050\n",
      "epoch[31\\200 loss: 2.811021\n",
      "epoch[32\\200 loss: 2.744595\n",
      "epoch[33\\200 loss: 2.732336\n",
      "epoch[34\\200 loss: 2.754540\n",
      "epoch[35\\200 loss: 2.745453\n",
      "epoch[36\\200 loss: 2.742124\n",
      "epoch[37\\200 loss: 2.725006\n",
      "epoch[38\\200 loss: 2.734377\n",
      "epoch[39\\200 loss: 2.768493\n",
      "epoch[40\\200 loss: 2.680614\n",
      "epoch[41\\200 loss: 2.742217\n",
      "epoch[42\\200 loss: 2.663044\n",
      "epoch[43\\200 loss: 2.725747\n",
      "epoch[44\\200 loss: 2.726831\n",
      "epoch[45\\200 loss: 2.718649\n",
      "epoch[46\\200 loss: 2.696894\n",
      "epoch[47\\200 loss: 2.671348\n",
      "epoch[48\\200 loss: 2.698055\n",
      "epoch[49\\200 loss: 2.683237\n",
      "epoch[50\\200 loss: 2.652203\n",
      "epoch[51\\200 loss: 2.643956\n",
      "epoch[52\\200 loss: 2.634760\n",
      "epoch[53\\200 loss: 2.714627\n",
      "epoch[54\\200 loss: 2.722004\n",
      "epoch[55\\200 loss: 2.663792\n",
      "epoch[56\\200 loss: 2.648385\n",
      "epoch[57\\200 loss: 2.657372\n",
      "epoch[58\\200 loss: 2.644238\n",
      "epoch[59\\200 loss: 2.603672\n",
      "epoch[60\\200 loss: 2.701738\n",
      "epoch[61\\200 loss: 2.665358\n",
      "epoch[62\\200 loss: 2.645811\n",
      "epoch[63\\200 loss: 2.671082\n",
      "epoch[64\\200 loss: 2.641752\n",
      "epoch[65\\200 loss: 2.639085\n",
      "epoch[66\\200 loss: 2.618739\n",
      "epoch[67\\200 loss: 2.633245\n",
      "epoch[68\\200 loss: 2.671972\n",
      "epoch[69\\200 loss: 2.657123\n",
      "epoch[70\\200 loss: 2.623479\n",
      "epoch[71\\200 loss: 2.644383\n",
      "epoch[72\\200 loss: 2.620210\n",
      "epoch[73\\200 loss: 2.622957\n",
      "epoch[74\\200 loss: 2.696214\n",
      "epoch[75\\200 loss: 2.692442\n",
      "epoch[76\\200 loss: 2.600774\n",
      "epoch[77\\200 loss: 2.580133\n",
      "epoch[78\\200 loss: 2.575078\n",
      "epoch[79\\200 loss: 2.622773\n",
      "epoch[80\\200 loss: 2.675109\n",
      "epoch[81\\200 loss: 2.583040\n",
      "epoch[82\\200 loss: 2.569989\n",
      "epoch[83\\200 loss: 2.664157\n",
      "epoch[84\\200 loss: 2.585239\n",
      "epoch[85\\200 loss: 2.609793\n",
      "epoch[86\\200 loss: 2.567146\n",
      "epoch[87\\200 loss: 2.545811\n",
      "epoch[88\\200 loss: 2.628742\n",
      "epoch[89\\200 loss: 2.534686\n",
      "epoch[90\\200 loss: 2.562031\n",
      "epoch[91\\200 loss: 2.606166\n",
      "epoch[92\\200 loss: 2.539517\n",
      "epoch[93\\200 loss: 2.568240\n",
      "epoch[94\\200 loss: 2.620522\n",
      "epoch[95\\200 loss: 2.541755\n",
      "epoch[96\\200 loss: 2.505177\n",
      "epoch[97\\200 loss: 2.567969\n",
      "epoch[98\\200 loss: 2.613029\n",
      "epoch[99\\200 loss: 2.541769\n",
      "epoch[100\\200 loss: 2.574207\n",
      "epoch[101\\200 loss: 2.547044\n",
      "epoch[102\\200 loss: 2.553344\n",
      "epoch[103\\200 loss: 2.521964\n",
      "epoch[104\\200 loss: 2.541436\n",
      "epoch[105\\200 loss: 2.551247\n",
      "epoch[106\\200 loss: 2.576329\n",
      "epoch[107\\200 loss: 2.560396\n",
      "epoch[108\\200 loss: 2.590123\n",
      "epoch[109\\200 loss: 2.577185\n",
      "epoch[110\\200 loss: 2.488980\n",
      "epoch[111\\200 loss: 2.540450\n",
      "epoch[112\\200 loss: 2.540031\n",
      "epoch[113\\200 loss: 2.544525\n",
      "epoch[114\\200 loss: 2.528765\n",
      "epoch[115\\200 loss: 2.522135\n",
      "epoch[116\\200 loss: 2.517344\n",
      "epoch[117\\200 loss: 2.475646\n",
      "epoch[118\\200 loss: 2.574073\n",
      "epoch[119\\200 loss: 2.500131\n",
      "epoch[120\\200 loss: 2.510288\n",
      "epoch[121\\200 loss: 2.547014\n",
      "epoch[122\\200 loss: 2.561160\n",
      "epoch[123\\200 loss: 2.474814\n",
      "epoch[124\\200 loss: 2.488963\n",
      "epoch[125\\200 loss: 2.547108\n",
      "epoch[126\\200 loss: 2.478024\n",
      "epoch[127\\200 loss: 2.482161\n",
      "epoch[128\\200 loss: 2.560380\n",
      "epoch[129\\200 loss: 2.462062\n",
      "epoch[130\\200 loss: 2.456056\n",
      "epoch[131\\200 loss: 2.477321\n",
      "epoch[132\\200 loss: 2.460147\n",
      "epoch[133\\200 loss: 2.489785\n",
      "epoch[134\\200 loss: 2.491149\n",
      "epoch[135\\200 loss: 2.499963\n",
      "epoch[136\\200 loss: 2.480147\n",
      "epoch[137\\200 loss: 2.475458\n",
      "epoch[138\\200 loss: 2.448280\n",
      "epoch[139\\200 loss: 2.491638\n",
      "epoch[140\\200 loss: 2.435146\n",
      "epoch[141\\200 loss: 2.468277\n",
      "epoch[142\\200 loss: 2.478037\n",
      "epoch[143\\200 loss: 2.478645\n",
      "epoch[144\\200 loss: 2.426765\n",
      "epoch[145\\200 loss: 2.463986\n",
      "epoch[146\\200 loss: 2.559376\n",
      "epoch[147\\200 loss: 2.491118\n",
      "epoch[148\\200 loss: 2.526986\n",
      "epoch[149\\200 loss: 2.474964\n",
      "epoch[150\\200 loss: 2.424982\n",
      "epoch[151\\200 loss: 2.515509\n",
      "epoch[152\\200 loss: 2.465043\n",
      "epoch[153\\200 loss: 2.450572\n",
      "epoch[154\\200 loss: 2.456705\n",
      "epoch[155\\200 loss: 2.469545\n",
      "epoch[156\\200 loss: 2.412408\n",
      "epoch[157\\200 loss: 2.490531\n",
      "epoch[158\\200 loss: 2.511669\n",
      "epoch[159\\200 loss: 2.468070\n",
      "epoch[160\\200 loss: 2.449908\n",
      "epoch[161\\200 loss: 2.457343\n",
      "epoch[162\\200 loss: 2.410373\n",
      "epoch[163\\200 loss: 2.480923\n",
      "epoch[164\\200 loss: 2.448113\n",
      "epoch[165\\200 loss: 2.410389\n",
      "epoch[166\\200 loss: 2.503700\n",
      "epoch[167\\200 loss: 2.426041\n",
      "epoch[168\\200 loss: 2.465896\n",
      "epoch[169\\200 loss: 2.434042\n",
      "epoch[170\\200 loss: 2.422895\n",
      "epoch[171\\200 loss: 2.470815\n",
      "epoch[172\\200 loss: 2.435892\n",
      "epoch[173\\200 loss: 2.418414\n",
      "epoch[174\\200 loss: 2.421007\n",
      "epoch[175\\200 loss: 2.436220\n",
      "epoch[176\\200 loss: 2.380887\n",
      "epoch[177\\200 loss: 2.430454\n",
      "epoch[178\\200 loss: 2.423397\n",
      "epoch[179\\200 loss: 2.425348\n",
      "epoch[180\\200 loss: 2.376394\n",
      "epoch[181\\200 loss: 2.435807\n",
      "epoch[182\\200 loss: 2.420977\n",
      "epoch[183\\200 loss: 2.370630\n",
      "epoch[184\\200 loss: 2.460037\n",
      "epoch[185\\200 loss: 2.476789\n",
      "epoch[186\\200 loss: 2.395621\n",
      "epoch[187\\200 loss: 2.398252\n",
      "epoch[188\\200 loss: 2.438040\n",
      "epoch[189\\200 loss: 2.455314\n",
      "epoch[190\\200 loss: 2.385862\n",
      "epoch[191\\200 loss: 2.424774\n",
      "epoch[192\\200 loss: 2.428873\n",
      "epoch[193\\200 loss: 2.353506\n",
      "epoch[194\\200 loss: 2.445202\n",
      "epoch[195\\200 loss: 2.388873\n",
      "epoch[196\\200 loss: 2.403579\n",
      "epoch[197\\200 loss: 2.400626\n",
      "epoch[198\\200 loss: 2.398932\n",
      "epoch[199\\200 loss: 2.484676\n",
      "epoch[200\\200 loss: 2.375474\n",
      "[10, 2, 12, 12, 6, 1, 17, 6, 9, 12, 11, 1, 3, 10, 2, 1, 3, 10, 2, 1, 3, 10, 2, 1, 3, 10, 2, 1, 3, 10, 2, 1, 3, 10, 2, 1, 3, 10, 2, 1, 8, 4, 9, 2, 1, 3, 10, 2, 1, 3, 10, 2, 1, 3, 10, 2, 1, 3, 10, 2, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello world the the the the the the the sare the the the the '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens = 256\n",
    "\n",
    "\n",
    "class RNN(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, hiddens, **kwargs):\n",
    "        super(RNN, self).__init__(**kwargs)\n",
    "        self.rnn_cell = tf.keras.layers.SimpleRNNCell(\n",
    "            hiddens, kernel_initializer='glorot_uniform')\n",
    "        self.rnn_layer = tf.keras.layers.RNN(self.rnn_cell,\n",
    "                                             time_major=True,\n",
    "                                             return_sequences=True,\n",
    "                                             return_state=True)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, x, state):\n",
    "        x = tf.one_hot(tf.transpose(x), self.vocab_size)\n",
    "        y, *state = self.rnn_layer(x, state)\n",
    "        y = self.dense(tf.reshape(y, (-1, y.shape[-1])))\n",
    "        return y, state\n",
    "\n",
    "    def predict(self, x, n_pred, vocab: Vocab):\n",
    "        pred_state = self.init_state(batch_size=1, dtype='float32')\n",
    "        y = [vocab[x[0]]]\n",
    "        # 先预热state，其实就是记录x的隐藏状态\n",
    "        for y_ in x[1:]:\n",
    "            _, pred_state = self.__call__(\n",
    "                tf.reshape(tf.constant(y[-1]), (1, 1)).numpy(), pred_state)\n",
    "            y.append(vocab[y_])\n",
    "        # 开始利用x的隐藏状态进行预测\n",
    "        for _ in range(n_pred):\n",
    "            y_, pred_state = self.__call__(\n",
    "                tf.reshape(tf.constant(y[-1]), (1, 1)).numpy(), pred_state)\n",
    "            y.append(y_.numpy().argmax(axis=1)[0])\n",
    "        print(y)\n",
    "        return ''.join([vocab.index_to_token[c] for c in y])\n",
    "    \n",
    "    def gradient_clip(self, grads, theta):\n",
    "        \"\"\"\n",
    "            梯度裁剪，防止梯度爆炸问题\n",
    "        \"\"\"\n",
    "        theta = tf.constant(theta, dtype=\"float32\")\n",
    "        new_grads = []\n",
    "        for grad in grads:\n",
    "            new_grads.append(\n",
    "                tf.convert_to_tensor(grad) if isinstance(\n",
    "                    grad, tf.IndexedSlices) else grad)\n",
    "        # L2范数\n",
    "        norm = tf.math.sqrt(\n",
    "            sum((tf.reduce_sum(grad**2).numpy() for grad in new_grads)))\n",
    "        norm = tf.cast(norm, \"float32\")\n",
    "        if tf.greater(norm, theta):\n",
    "            for i, grad in enumerate(new_grads):\n",
    "                new_grads[i] = grad * theta / norm\n",
    "        return new_grads\n",
    "\n",
    "    def fit(self, train_iter, epochs=10, lr=1e-3):\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n",
    "        for epoch in range(epochs):\n",
    "            for x, y in train_iter():\n",
    "                state = self.init_state(batch_size=x.shape[0], dtype='float32')\n",
    "                with tf.GradientTape(persistent=True) as gt:\n",
    "                    # 向前计算\n",
    "                    y_hat, state = self.__call__(x, state)\n",
    "                    y = tf.reshape(tf.transpose(y), (-1, 1))\n",
    "                    # 损失计算\n",
    "                    l = loss_fn(y, y_hat)\n",
    "                grads = gt.gradient(l, self.trainable_variables)\n",
    "                grads = self.gradient_clip(grads, 1)\n",
    "                # print(tf.reduce_sum(grads[0]))\n",
    "                optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "            print(\"epoch[%d\\%d loss: %.6f\" % (epoch + 1, epochs, l))\n",
    "    \n",
    "    def init_state(self, *args, **kwargs):\n",
    "        return self.rnn_cell.get_initial_state(*args, **kwargs)\n",
    "\n",
    "net = RNN(len(vocab), 512)\n",
    "def get_train_iter():\n",
    "    return seq_data_iter_random(corpus=corpus,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  num_steps=NUM_STEPS)\n",
    "net.fit(train_iter=get_train_iter, epochs=200, lr=1e-3)\n",
    "net.predict('hello world', 50, vocab)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ae33f7c48cc3e1271596d1bf08ce4d5e6d6f7129ff8bbb83bbb95ed8addff62"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
