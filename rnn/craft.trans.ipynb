{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import zhconv\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        counter = Vocab.count_corpus(tokens)\n",
    "        # 对词频率排序\n",
    "        self.__token_freqs = sorted(counter.items(),\n",
    "                                    key=lambda x: x[1],\n",
    "                                    reverse=True)\n",
    "        self.index_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_index = {\n",
    "            token: idx\n",
    "            for idx, token in enumerate(self.index_to_token)\n",
    "        }\n",
    "        for token, freq in self.__token_freqs:\n",
    "            if freq >= min_freq and token not in self.token_to_index:\n",
    "                self.index_to_token.append(token)\n",
    "                self.token_to_index[token] = len(self.index_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_to_token)\n",
    "\n",
    "    def get_tokens(self, indicates):\n",
    "        if not isinstance(indicates, (list, tuple)):\n",
    "            return self.index_to_token[indicates]\n",
    "        return ''.join([self.get_tokens(index) for index in indicates])\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_index.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self.__token_freqs\n",
    "\n",
    "    @staticmethod\n",
    "    def count_corpus(tokens):\n",
    "        if isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        from collections import Counter\n",
    "        return Counter(tokens)\n",
    "\n",
    "\n",
    "def truncate_and_pad(line, steps, padding_token):\n",
    "    \"\"\"\n",
    "    \\{填充||截断\\}序列，使序列长度保持一致\n",
    "    \"\"\"\n",
    "    if len(line) > steps:\n",
    "        return line[:steps]\n",
    "    return line + [padding_token] * (steps - len(line))\n",
    "\n",
    "\n",
    "def load_datasets(steps=20, batch_size=50):\n",
    "    \"\"\"\n",
    "    预处理数据并封装成tf.data.Dataset\n",
    "    \"\"\"\n",
    "    with open('./en_zh.trans.txt', 'r') as data_file:\n",
    "        lines = data_file.readlines()\n",
    "    # print(zhconv.convert(s[1], 'zh-cn'))\n",
    "    en, zh = [], []\n",
    "    for line in lines:\n",
    "        split = line.split('\\t')\n",
    "        en.append(\n",
    "            truncate_and_pad(\n",
    "                re.sub('[^A-Za-z]+', ' ', split[0]).strip().lower().split(' ')\n",
    "                + ['<eos>'], steps, '<pad>')), zh.append(\n",
    "                    truncate_and_pad(\n",
    "                        list(\n",
    "                            jieba.cut(zhconv.convert(split[1], 'zh-cn'),\n",
    "                                      cut_all=False)) + ['<eos>'], steps,\n",
    "                    '<pad>'))\n",
    "    en_vocab, zh_vocab = Vocab(\n",
    "        en, min_freq=2,\n",
    "        reserved_tokens=['<pad>', '<bos>', '<eos>'\n",
    "                         ]), Vocab(zh,\n",
    "                                   min_freq=2,\n",
    "                                   reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    en = tf.constant([en_vocab[line] for line in en], dtype='float32')\n",
    "    zh = tf.constant([zh_vocab[line] for line in zh], dtype='float32')\n",
    "    en_len = tf.reduce_sum(tf.cast(en != en_vocab['<pad>'], dtype='float32'), axis=1)\n",
    "    zh_len = tf.reduce_sum(tf.cast(zh != zh_vocab['<pad>'], dtype='float32'), axis=1)\n",
    "    ds = (\n",
    "        tf.data.Dataset.from_tensor_slices(en),\n",
    "        tf.data.Dataset.from_tensor_slices(en_len),\n",
    "        tf.data.Dataset.from_tensor_slices(zh),\n",
    "        tf.data.Dataset.from_tensor_slices(zh_len),\n",
    "    )\n",
    "    train_iter = tf.data.Dataset.zip(ds).shuffle(buffer_size=len(en)).batch(batch_size=batch_size)\n",
    "    return train_iter, en_vocab, zh_vocab\n",
    "\n",
    "train_iter, en_vocab, zh_vocab = load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hiddens, layers, dropout=0., **kwargs):\n",
    "        super.__init__(Seq2SeqEncoder, **kwargs)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
    "        self.rnn_net = tf.keras.layers.StackedRNNCells([\n",
    "                tf.keras.layers.GRUCell(hiddens, dropout=dropout) for _ in range(layers)\n",
    "            ], return_sequences=True, return_state=True)\n",
    "    \n",
    "    def call(self, x, *args, **kwargs):\n",
    "        x = self.embedding(x)\n",
    "        y = self.rnn_net(x, *args, **kwargs)\n",
    "        state = y[1:]\n",
    "        return y[0], state\n",
    "\n",
    "class Seq2SeqDecoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hiddens, layers, dropout=0., **kwargs):\n",
    "        super.__init__(Seq2SeqDecoder, **kwargs)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
    "        self.rnn_net = tf.keras.layers.StackedRNNCells([\n",
    "            tf.keras.layers.GRUCell(hiddens, dropout=dropout) for _ in range(layers)\n",
    "            ], return_sequences=True, return_state=True)\n",
    "        self.output = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x, state, **kwargs):\n",
    "        x = self.embedding(x)\n",
    "        context = tf.repeat(tf.expand_dims(state[-1], axis=1), repeats=x.shape[1], axis=1)\n",
    "        x_ctx = tf.concat((x, context), axis=2)\n",
    "        y = self.rnn_net(x, state, **kwargs)\n",
    "        state = y[1:]\n",
    "        y = self.output(y[0])\n",
    "        return y, state"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ae33f7c48cc3e1271596d1bf08ce4d5e6d6f7129ff8bbb83bbb95ed8addff62"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
