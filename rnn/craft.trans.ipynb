{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import zhconv\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "\n",
    "class Vocab:\n",
    "\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        counter = Vocab.count_corpus(tokens)\n",
    "        # 对词频率排序\n",
    "        self.__token_freqs = sorted(counter.items(),\n",
    "                                    key=lambda x: x[1],\n",
    "                                    reverse=True)\n",
    "        self.index_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_index = {\n",
    "            token: idx\n",
    "            for idx, token in enumerate(self.index_to_token)\n",
    "        }\n",
    "        for token, freq in self.__token_freqs:\n",
    "            if freq >= min_freq and token not in self.token_to_index:\n",
    "                self.index_to_token.append(token)\n",
    "                self.token_to_index[token] = len(self.index_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_to_token)\n",
    "\n",
    "    def get_tokens(self, indicates):\n",
    "        if not isinstance(indicates, (list, tuple)):\n",
    "            return self.index_to_token[indicates]\n",
    "        return ''.join([self.get_tokens(index) for index in indicates])\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_index.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self.__token_freqs\n",
    "\n",
    "    @staticmethod\n",
    "    def count_corpus(tokens):\n",
    "        if isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        from collections import Counter\n",
    "        return Counter(tokens)\n",
    "\n",
    "\n",
    "def truncate_and_pad(line, steps, padding_token):\n",
    "    \"\"\"\n",
    "    \\{填充||截断\\}序列，使序列长度保持一致\n",
    "    \"\"\"\n",
    "    if len(line) > steps:\n",
    "        return line[:steps]\n",
    "    return line + [padding_token] * (steps - len(line))\n",
    "\n",
    "\n",
    "def load_datasets(steps=20, batch_size=50):\n",
    "    \"\"\"\n",
    "    预处理数据并封装成tf.data.Dataset\n",
    "    \"\"\"\n",
    "    with open('./en_zh.trans.txt', 'r') as data_file:\n",
    "        lines = data_file.readlines()\n",
    "    # print(zhconv.convert(s[1], 'zh-cn'))\n",
    "    en, zh = [], []\n",
    "    for line in lines:\n",
    "        split = line.split('\\t')\n",
    "        en.append(\n",
    "            truncate_and_pad(\n",
    "                re.sub('[^A-Za-z]+', ' ', split[0]).strip().lower().split(' ')\n",
    "                + ['<eos>'], steps, '<pad>')), zh.append(\n",
    "                    truncate_and_pad(\n",
    "                        list(\n",
    "                            jieba.cut(zhconv.convert(split[1], 'zh-cn'),\n",
    "                                      cut_all=False)) + ['<eos>'], steps,\n",
    "                    '<pad>'))\n",
    "    en_vocab, zh_vocab = Vocab(\n",
    "        en, min_freq=2,\n",
    "        reserved_tokens=['<pad>', '<bos>', '<eos>'\n",
    "                         ]), Vocab(zh,\n",
    "                                   min_freq=2,\n",
    "                                   reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "    en = tf.constant([en_vocab[line] for line in en], dtype='float32')\n",
    "    zh = tf.constant([zh_vocab[line] for line in zh], dtype='int32')\n",
    "    en_len = tf.reduce_sum(tf.cast(en != en_vocab['<pad>'], dtype='int32'), axis=1)\n",
    "    zh_len = tf.reduce_sum(tf.cast(zh != zh_vocab['<pad>'], dtype='int32'), axis=1)\n",
    "    ds = (\n",
    "        tf.data.Dataset.from_tensor_slices(en),\n",
    "        tf.data.Dataset.from_tensor_slices(en_len),\n",
    "        tf.data.Dataset.from_tensor_slices(zh),\n",
    "        tf.data.Dataset.from_tensor_slices(zh_len),\n",
    "    )\n",
    "    # en.shape=(batch_size, steps)\n",
    "    train_iter = tf.data.Dataset.zip(ds).shuffle(buffer_size=len(en)).batch(batch_size=batch_size)\n",
    "    return train_iter, en_vocab, zh_vocab\n",
    "\n",
    "BATCH_SIZE, STEPS = 64, 20\n",
    "train_iter, en_vocab, zh_vocab = load_datasets(batch_size=BATCH_SIZE, steps=STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqEncoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hiddens, layers, dropout=0., **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
    "        self.rnn_net = tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells([\n",
    "                tf.keras.layers.GRUCell(hiddens, dropout=dropout) for _ in range(layers)\n",
    "            ]), return_sequences=True, return_state=True)\n",
    "    \n",
    "    def call(self, x, *args, **kwargs):\n",
    "        x = self.embedding(x)\n",
    "        y = self.rnn_net(x, *args, **kwargs)\n",
    "        state = y[1:]\n",
    "        return y[0], state\n",
    "\n",
    "class Seq2SeqDecoder(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size, hiddens, layers, dropout=0., **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embed_size)\n",
    "        self.rnn_net = tf.keras.layers.RNN(tf.keras.layers.StackedRNNCells([\n",
    "            tf.keras.layers.GRUCell(hiddens, dropout=dropout) for _ in range(layers)\n",
    "            ]), return_sequences=True, return_state=True)\n",
    "        self.output_layer = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def init_state(self, encode, *args):\n",
    "        return encode[1]\n",
    "        \n",
    "    def call(self, x, state, **kwargs):\n",
    "        x = self.embedding(x)\n",
    "        context = tf.repeat(tf.expand_dims(state[-1], axis=1), repeats=x.shape[1], axis=1)\n",
    "        x_ctx = tf.concat((x, context), axis=2)\n",
    "        y = self.rnn_net(x, state, **kwargs)\n",
    "        state = y[1:]\n",
    "        y = self.output_layer(y[0])\n",
    "        return y, state\n",
    "\n",
    "class EncoderDecoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def call(self, encode_x, decode_x, *args, **kwargs):\n",
    "        encode = self.encoder(encode_x, *args, **kwargs)\n",
    "        state = self.decoder.init_state(encode, *args)\n",
    "        return self.decoder(decode_x, state, **kwargs)\n",
    "    \n",
    "    def fit(self, train_iter, target_vocab, epochs=10, lr=1e-3):\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "        for epoch in range(epochs):\n",
    "            for step, (x, xl, y, yl) in enumerate(train_iter):\n",
    "                bos = tf.reshape(tf.constant([target_vocab['<bos>']] * y.shape[0]), shape=(-1, 1))\n",
    "                decode_x = tf.concat([bos, tf.cast(y[:, :-1], dtype='int32')] , 1)  # 给y加入<bos> 忽略末尾的<eos>\n",
    "                with tf.GradientTape() as gt:\n",
    "                    y_hat, _ = self.call(x, decode_x, training=True)\n",
    "                    loss = MaskedSoftmaxLoss(yl)(y, y_hat)\n",
    "                grads = gt.gradient(self.trainable_variables)\n",
    "                grads = self.gradient_clip(grads)\n",
    "                optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "            print(f'epoch[{epoch + 1}\\\\{epochs} loss: {tf.reduce_mean(loss)}')\n",
    "    \n",
    "    def gradient_clip(self, grads, theta):\n",
    "        \"\"\"\n",
    "            梯度裁剪，防止梯度爆炸问题\n",
    "        \"\"\"\n",
    "        theta = tf.constant(theta, dtype=\"float32\")\n",
    "        new_grads = []\n",
    "        for grad in grads:\n",
    "            new_grads.append(\n",
    "                tf.convert_to_tensor(grad) if isinstance(\n",
    "                    grad, tf.IndexedSlices) else grad)\n",
    "        # L2范数\n",
    "        norm = tf.math.sqrt(\n",
    "            sum((tf.reduce_sum(grad**2).numpy() for grad in new_grads)))\n",
    "        norm = tf.cast(norm, \"float32\")\n",
    "        if tf.greater(norm, theta):\n",
    "            for i, grad in enumerate(new_grads):\n",
    "                new_grads[i] = grad * theta / norm\n",
    "        return new_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSoftmaxLoss(tf.keras.losses.Loss):\n",
    "    \n",
    "    def __init__(self, valid_len):\n",
    "        super().__init__(reduction='none')\n",
    "        self.valid_len = valid_len\n",
    "        self._loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    \n",
    "    def call(self, y, y_hat):\n",
    "        mask = tf.ones_like(y, dtype='float32')\n",
    "        mask = self.sequence_mask(mask)\n",
    "        y = tf.one_hot(y, depth=y_hat.shape[-1])\n",
    "        _loss = self._loss_fn(y, y_hat)\n",
    "        return tf.reduce_mean(mask * _loss, axis=1)\n",
    "\n",
    "    def sequence_mask(self, x, value=0):\n",
    "        \"\"\"\n",
    "        由于做了固定长度的填充，在预测时要忽略掉\n",
    "        \"\"\"\n",
    "        # x.shape=(batch_size, steps)\n",
    "        # xl.shape=(steps, )\n",
    "        # (1, 32) (32, 1)\n",
    "        mask = tf.range(start=0, limit=x.shape[1],\n",
    "                        dtype='float32')[None, :] < tf.cast(self.valid_len[:, None],\n",
    "                                                            dtype='float32')\n",
    "        if len(x.shape) == 3:\n",
    "            return tf.where(tf.expand_dims(mask, axis=-1), x, value)\n",
    "        else:\n",
    "            return  tf.where(mask, x, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE, HIDDENTS, LAYERS, DROPOUT = 32, 32, 2, .1\n",
    "\n",
    "encoder = Seq2SeqEncoder(len(en_vocab), EMBED_SIZE, HIDDENTS, LAYERS, DROPOUT)\n",
    "decoder = Seq2SeqDecoder(len(zh_vocab), EMBED_SIZE, HIDDENTS, LAYERS, DROPOUT)\n",
    "\n",
    "net = EncoderDecoder(encoder=encoder, decoder=decoder)\n",
    "net.fit(train_iter, zh_vocab, epochs=10, lr=1e-3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ae33f7c48cc3e1271596d1bf08ce4d5e6d6f7129ff8bbb83bbb95ed8addff62"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
