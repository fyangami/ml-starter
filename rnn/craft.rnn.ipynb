{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    with open('./the_time_machine.txt', 'r') as txt:\n",
    "        lines = txt.readlines()\n",
    "    import re\n",
    "    return [\n",
    "        l for l in\n",
    "        [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "        if l.strip() != ''\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3093"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lines, token='word'):\n",
    "    return [list(line) if token == 'char' else line.split() for line in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "\n",
    "    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n",
    "        counter = Vocab.count_corpus(tokens)\n",
    "        # 对词频率排序\n",
    "        self.__token_freqs = sorted(counter.items(),\n",
    "                                    key=lambda x: x[1],\n",
    "                                    reverse=True)\n",
    "        self.index_to_token = ['<unk>'] + reserved_tokens\n",
    "        self.token_to_index = {\n",
    "            token: idx\n",
    "            for idx, token in enumerate(self.index_to_token)\n",
    "        }\n",
    "        for token, freq in self.__token_freqs:\n",
    "            if freq >= min_freq and token not in self.token_to_index:\n",
    "                self.index_to_token.append(token)\n",
    "                self.token_to_index[token] = len(self.index_to_token) - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_to_token)\n",
    "\n",
    "    def get_tokens(self, indicates):\n",
    "        if not isinstance(indicates, (list, tuple)):\n",
    "            return self.index_to_token[indicates]\n",
    "        return ''.join([self.get_tokens(index) for index in indicates])\n",
    "    \n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_index.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def token_freqs(self):\n",
    "        return self.__token_freqs\n",
    "\n",
    "    @staticmethod\n",
    "    def count_corpus(tokens):\n",
    "        if isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        from collections import Counter\n",
    "        return Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(lines)\n",
    "vocab = Vocab(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2477),\n",
       " ('and', 1312),\n",
       " ('of', 1286),\n",
       " ('i', 1268),\n",
       " ('a', 877),\n",
       " ('to', 766),\n",
       " ('in', 606),\n",
       " ('was', 554),\n",
       " ('that', 458),\n",
       " ('it', 452),\n",
       " ('my', 441),\n",
       " ('had', 354),\n",
       " ('as', 281),\n",
       " ('me', 281),\n",
       " ('with', 264),\n",
       " ('at', 257),\n",
       " ('for', 247),\n",
       " ('you', 212),\n",
       " ('time', 211),\n",
       " ('but', 209),\n",
       " ('this', 199),\n",
       " ('or', 162),\n",
       " ('were', 158),\n",
       " ('on', 148),\n",
       " ('not', 142),\n",
       " ('from', 137),\n",
       " ('all', 136),\n",
       " ('then', 134),\n",
       " ('is', 129),\n",
       " ('have', 129),\n",
       " ('his', 129),\n",
       " ('there', 128),\n",
       " ('by', 126),\n",
       " ('he', 126),\n",
       " ('they', 124),\n",
       " ('one', 120),\n",
       " ('upon', 115),\n",
       " ('so', 114),\n",
       " ('into', 114),\n",
       " ('little', 114),\n",
       " ('be', 112),\n",
       " ('came', 107),\n",
       " ('no', 102),\n",
       " ('gutenberg', 98),\n",
       " ('some', 95),\n",
       " ('machine', 93),\n",
       " ('could', 93),\n",
       " ('an', 92),\n",
       " ('which', 92),\n",
       " ('we', 91),\n",
       " ('their', 91),\n",
       " ('said', 89),\n",
       " ('project', 88),\n",
       " ('saw', 88),\n",
       " ('down', 87),\n",
       " ('s', 86),\n",
       " ('very', 86),\n",
       " ('them', 86),\n",
       " ('now', 79),\n",
       " ('what', 78),\n",
       " ('these', 77),\n",
       " ('about', 77),\n",
       " ('any', 75),\n",
       " ('been', 75),\n",
       " ('her', 75),\n",
       " ('up', 74),\n",
       " ('out', 74),\n",
       " ('like', 74),\n",
       " ('if', 72),\n",
       " ('its', 72),\n",
       " ('seemed', 72),\n",
       " ('are', 71),\n",
       " ('man', 71),\n",
       " ('thing', 66),\n",
       " ('traveller', 65),\n",
       " ('again', 62),\n",
       " ('white', 61),\n",
       " ('our', 61),\n",
       " ('more', 60),\n",
       " ('would', 60),\n",
       " ('must', 59),\n",
       " ('when', 58),\n",
       " ('thought', 57),\n",
       " ('felt', 57),\n",
       " ('tm', 57),\n",
       " ('work', 55),\n",
       " ('weena', 54),\n",
       " ('other', 53),\n",
       " ('even', 53),\n",
       " ('still', 53),\n",
       " ('before', 52),\n",
       " ('over', 52),\n",
       " ('through', 51),\n",
       " ('myself', 51),\n",
       " ('people', 49),\n",
       " ('hand', 49),\n",
       " ('went', 49),\n",
       " ('first', 49),\n",
       " ('may', 48),\n",
       " ('morlocks', 48),\n",
       " ('only', 48),\n",
       " ('see', 48),\n",
       " ('last', 47),\n",
       " ('how', 47),\n",
       " ('towards', 47),\n",
       " ('found', 47),\n",
       " ('will', 46),\n",
       " ('she', 46),\n",
       " ('too', 45),\n",
       " ('here', 43),\n",
       " ('light', 43),\n",
       " ('great', 42),\n",
       " ('did', 41),\n",
       " ('away', 40),\n",
       " ('him', 40),\n",
       " ('way', 40),\n",
       " ('began', 40),\n",
       " ('back', 40),\n",
       " ('world', 39),\n",
       " ('under', 39),\n",
       " ('after', 39),\n",
       " ('do', 39),\n",
       " ('night', 38),\n",
       " ('face', 38),\n",
       " ('such', 38),\n",
       " ('same', 38),\n",
       " ('than', 37),\n",
       " ('another', 37),\n",
       " ('well', 37),\n",
       " ('where', 36),\n",
       " ('us', 36),\n",
       " ('things', 36),\n",
       " ('think', 36),\n",
       " ('round', 36),\n",
       " ('made', 36),\n",
       " ('eyes', 35),\n",
       " ('mind', 35),\n",
       " ('long', 35),\n",
       " ('might', 35),\n",
       " ('perhaps', 35),\n",
       " ('put', 34),\n",
       " ('looked', 34),\n",
       " ('own', 34),\n",
       " ('most', 33),\n",
       " ('against', 33),\n",
       " ('among', 33),\n",
       " ('sky', 33),\n",
       " ('can', 32),\n",
       " ('took', 32),\n",
       " ('day', 32),\n",
       " ('strange', 32),\n",
       " ('yet', 32),\n",
       " ('works', 32),\n",
       " ('new', 31),\n",
       " ('moment', 31),\n",
       " ('old', 31),\n",
       " ('come', 31),\n",
       " ('sun', 31),\n",
       " ('fire', 30),\n",
       " ('know', 30),\n",
       " ('black', 30),\n",
       " ('darkness', 29),\n",
       " ('who', 29),\n",
       " ('green', 28),\n",
       " ('two', 28),\n",
       " ('t', 28),\n",
       " ('enough', 28),\n",
       " ('off', 28),\n",
       " ('hands', 28),\n",
       " ('presently', 28),\n",
       " ('place', 27),\n",
       " ('dark', 27),\n",
       " ('once', 27),\n",
       " ('electronic', 27),\n",
       " ('red', 26),\n",
       " ('end', 26),\n",
       " ('left', 26),\n",
       " ('grew', 26),\n",
       " ('full', 26),\n",
       " ('almost', 25),\n",
       " ('psychologist', 25),\n",
       " ('three', 25),\n",
       " ('space', 25),\n",
       " ('got', 25),\n",
       " ('should', 25),\n",
       " ('part', 25),\n",
       " ('foundation', 25),\n",
       " ('terms', 24),\n",
       " ('sphinx', 24),\n",
       " ('much', 24),\n",
       " ('has', 24),\n",
       " ('looking', 24),\n",
       " ('medical', 24),\n",
       " ('future', 24),\n",
       " ('stood', 24),\n",
       " ('fear', 24),\n",
       " ('above', 23),\n",
       " ('make', 23),\n",
       " ('air', 23),\n",
       " ('head', 23),\n",
       " ('e', 23),\n",
       " ('set', 22),\n",
       " ('sat', 22),\n",
       " ('without', 22),\n",
       " ('tried', 22),\n",
       " ('cannot', 22),\n",
       " ('far', 22),\n",
       " ('seen', 22),\n",
       " ('minute', 22),\n",
       " ('suddenly', 22),\n",
       " ('across', 22),\n",
       " ('soon', 21),\n",
       " ('along', 21),\n",
       " ('kind', 21),\n",
       " ('get', 21),\n",
       " ('earth', 21),\n",
       " ('turned', 21),\n",
       " ('while', 21),\n",
       " ('human', 21),\n",
       " ('use', 20),\n",
       " ('indeed', 20),\n",
       " ('heard', 20),\n",
       " ('say', 20),\n",
       " ('gone', 20),\n",
       " ('never', 20),\n",
       " ('until', 20),\n",
       " ('look', 20),\n",
       " ('certain', 20),\n",
       " ('already', 20),\n",
       " ('half', 20),\n",
       " ('editor', 20),\n",
       " ('feeling', 20),\n",
       " ('hill', 20),\n",
       " ('copyright', 20),\n",
       " ('states', 19),\n",
       " ('though', 19),\n",
       " ('go', 19),\n",
       " ('room', 19),\n",
       " ('laboratory', 19),\n",
       " ('those', 19),\n",
       " ('bronze', 19),\n",
       " ('flowers', 19),\n",
       " ('match', 19),\n",
       " ('matches', 19),\n",
       " ('gallery', 19),\n",
       " ('license', 18),\n",
       " ('rather', 18),\n",
       " ('right', 18),\n",
       " ('years', 18),\n",
       " ('feet', 18),\n",
       " ('moon', 18),\n",
       " ('agreement', 18),\n",
       " ('palace', 17),\n",
       " ('story', 17),\n",
       " ('filby', 17),\n",
       " ('ground', 17),\n",
       " ('however', 17),\n",
       " ('just', 17),\n",
       " ('your', 17),\n",
       " ('table', 17),\n",
       " ('behind', 17),\n",
       " ('lever', 17),\n",
       " ('big', 17),\n",
       " ('life', 17),\n",
       " ('ran', 17),\n",
       " ('door', 17),\n",
       " ('days', 17),\n",
       " ('united', 16),\n",
       " ('age', 16),\n",
       " ('soft', 16),\n",
       " ('mere', 16),\n",
       " ('don', 16),\n",
       " ('between', 16),\n",
       " ('each', 16),\n",
       " ('why', 16),\n",
       " ('fell', 16),\n",
       " ('morning', 16),\n",
       " ('something', 16),\n",
       " ('going', 16),\n",
       " ('past', 16),\n",
       " ('followed', 16),\n",
       " ('lay', 16),\n",
       " ('less', 16),\n",
       " ('lit', 16),\n",
       " ('four', 15),\n",
       " ('dimensions', 15),\n",
       " ('side', 15),\n",
       " ('whole', 15),\n",
       " ('small', 15),\n",
       " ('bright', 15),\n",
       " ('knew', 15),\n",
       " ('struck', 15),\n",
       " ('running', 15),\n",
       " ('creatures', 15),\n",
       " ('find', 15),\n",
       " ('wood', 15),\n",
       " ('donations', 15),\n",
       " ('travelling', 14),\n",
       " ('being', 14),\n",
       " ('large', 14),\n",
       " ('men', 14),\n",
       " ('odd', 14),\n",
       " ('understand', 14),\n",
       " ('good', 14),\n",
       " ('told', 14),\n",
       " ('forth', 14),\n",
       " ('next', 14),\n",
       " ('feel', 14),\n",
       " ('ever', 14),\n",
       " ('bushes', 14),\n",
       " ('coming', 14),\n",
       " ('literary', 14),\n",
       " ('trademark', 14),\n",
       " ('ebook', 13),\n",
       " ('sudden', 13),\n",
       " ('passed', 13),\n",
       " ('dinner', 13),\n",
       " ('course', 13),\n",
       " ('clearly', 13),\n",
       " ('lamp', 13),\n",
       " ('clear', 13),\n",
       " ('others', 13),\n",
       " ('rose', 13),\n",
       " ('move', 13),\n",
       " ('tell', 13),\n",
       " ('second', 13),\n",
       " ('silent', 13),\n",
       " ('times', 13),\n",
       " ('hundred', 13),\n",
       " ('doubt', 13),\n",
       " ('suppose', 13),\n",
       " ('dust', 13),\n",
       " ('nothing', 13),\n",
       " ('every', 13),\n",
       " ('dim', 13),\n",
       " ('open', 13),\n",
       " ('huge', 13),\n",
       " ('hall', 13),\n",
       " ('sea', 13),\n",
       " ('archive', 13),\n",
       " ('copy', 12),\n",
       " ('porcelain', 12),\n",
       " ('because', 12),\n",
       " ('am', 12),\n",
       " ('several', 12),\n",
       " ('vanished', 12),\n",
       " ('since', 12),\n",
       " ('below', 12),\n",
       " ('remember', 12),\n",
       " ('gave', 12),\n",
       " ('sound', 12),\n",
       " ('happened', 12),\n",
       " ('trees', 12),\n",
       " ('creature', 12),\n",
       " ('vast', 12),\n",
       " ('beautiful', 12),\n",
       " ('pocket', 12),\n",
       " ('nature', 12),\n",
       " ('within', 12),\n",
       " ('wells', 11),\n",
       " ('further', 11),\n",
       " ('pale', 11),\n",
       " ('grey', 11),\n",
       " ('line', 11),\n",
       " ('really', 11),\n",
       " ('except', 11),\n",
       " ('simply', 11),\n",
       " ('figure', 11),\n",
       " ('state', 11),\n",
       " ('eight', 11),\n",
       " ('done', 11),\n",
       " ('save', 11),\n",
       " ('possibly', 11),\n",
       " ('absolutely', 11),\n",
       " ('conditions', 11),\n",
       " ('sense', 11),\n",
       " ('queer', 11),\n",
       " ('evening', 11),\n",
       " ('either', 11),\n",
       " ('hesitated', 11),\n",
       " ('glass', 11),\n",
       " ('stars', 11),\n",
       " ('growing', 11),\n",
       " ('pedestal', 11),\n",
       " ('appeared', 11),\n",
       " ('thick', 11),\n",
       " ('altogether', 11),\n",
       " ('judged', 11),\n",
       " ('few', 11),\n",
       " ('blackness', 11),\n",
       " ('camphor', 11),\n",
       " ('slower', 11),\n",
       " ('paragraph', 11),\n",
       " ('f', 11),\n",
       " ('www', 10),\n",
       " ('org', 10),\n",
       " ('laws', 10),\n",
       " ('caught', 10),\n",
       " ('instance', 10),\n",
       " ('wrong', 10),\n",
       " ('idea', 10),\n",
       " ('always', 10),\n",
       " ('quite', 10),\n",
       " ('exactly', 10),\n",
       " ('animal', 10),\n",
       " ('better', 10),\n",
       " ('let', 10),\n",
       " ('walked', 10),\n",
       " ('larger', 10),\n",
       " ('m', 10),\n",
       " ('believe', 10),\n",
       " ('travelled', 10),\n",
       " ('bars', 10),\n",
       " ('met', 10),\n",
       " ('watch', 10),\n",
       " ('stopped', 10),\n",
       " ('sleep', 10),\n",
       " ('peculiar', 10),\n",
       " ('house', 10),\n",
       " ('humanity', 10),\n",
       " ('lawn', 10),\n",
       " ('creeping', 10),\n",
       " ('imagine', 10),\n",
       " ('memory', 10),\n",
       " ('number', 10),\n",
       " ('broken', 10),\n",
       " ('many', 10),\n",
       " ('strong', 10),\n",
       " ('determined', 10),\n",
       " ('physical', 10),\n",
       " ('alone', 10),\n",
       " ('ruins', 10),\n",
       " ('comfort', 10),\n",
       " ('including', 10),\n",
       " ('underworld', 10),\n",
       " ('arms', 10),\n",
       " ('iron', 10),\n",
       " ('access', 10),\n",
       " ('refund', 10),\n",
       " ('sunset', 9),\n",
       " ('explanation', 9),\n",
       " ('return', 9),\n",
       " ('free', 9),\n",
       " ('follow', 9),\n",
       " ('shall', 9),\n",
       " ('real', 9),\n",
       " ('nor', 9),\n",
       " ('explain', 9),\n",
       " ('ago', 9),\n",
       " ('moving', 9),\n",
       " ('travel', 9),\n",
       " ('means', 9),\n",
       " ('interest', 9),\n",
       " ('arm', 9),\n",
       " ('shoulder', 9),\n",
       " ('bar', 9),\n",
       " ('laughed', 9),\n",
       " ('stared', 9),\n",
       " ('nearly', 9),\n",
       " ('moved', 9),\n",
       " ('point', 9),\n",
       " ('plain', 9),\n",
       " ('none', 9),\n",
       " ('standing', 9),\n",
       " ('journalist', 9),\n",
       " ('opened', 9),\n",
       " ('brown', 9),\n",
       " ('faint', 9),\n",
       " ('question', 9),\n",
       " ('change', 9),\n",
       " ('rest', 9),\n",
       " ('apparently', 9),\n",
       " ('blue', 9),\n",
       " ('buildings', 9),\n",
       " ('hung', 9),\n",
       " ('stone', 9),\n",
       " ('lost', 9),\n",
       " ('fancied', 9),\n",
       " ('intellectual', 9),\n",
       " ('entered', 9),\n",
       " ('near', 9),\n",
       " ('necessity', 9),\n",
       " ('form', 9),\n",
       " ('strength', 9),\n",
       " ('social', 9),\n",
       " ('perfect', 9),\n",
       " ('doors', 9),\n",
       " ('associated', 9),\n",
       " ('section', 9),\n",
       " ('agree', 9),\n",
       " ('give', 8),\n",
       " ('shone', 8),\n",
       " ('need', 8),\n",
       " ('length', 8),\n",
       " ('became', 8),\n",
       " ('fact', 8),\n",
       " ('young', 8),\n",
       " ('surface', 8),\n",
       " ('telling', 8),\n",
       " ('different', 8),\n",
       " ('forward', 8),\n",
       " ('hope', 8),\n",
       " ('model', 8),\n",
       " ('also', 8),\n",
       " ('together', 8),\n",
       " ('motion', 8),\n",
       " ('faster', 8),\n",
       " ('itself', 8),\n",
       " ('beside', 8),\n",
       " ('take', 8),\n",
       " ('eye', 8),\n",
       " ('wanted', 8),\n",
       " ('colour', 8),\n",
       " ('cut', 8),\n",
       " ('faces', 8),\n",
       " ('meat', 8),\n",
       " ('thinking', 8),\n",
       " ('noticed', 8),\n",
       " ('assured', 8),\n",
       " ('horrible', 8),\n",
       " ('swiftly', 8),\n",
       " ('unknown', 8),\n",
       " ('everything', 8),\n",
       " ('drove', 8),\n",
       " ('instead', 8),\n",
       " ('race', 8),\n",
       " ('remote', 8),\n",
       " ('figures', 8),\n",
       " ('pretty', 8),\n",
       " ('thousand', 8),\n",
       " ('children', 8),\n",
       " ('loose', 8),\n",
       " ('received', 8),\n",
       " ('building', 8),\n",
       " ('general', 8),\n",
       " ('floor', 8),\n",
       " ('least', 8),\n",
       " ('view', 8),\n",
       " ('abundant', 8),\n",
       " ('security', 8),\n",
       " ('needs', 8),\n",
       " ('triumph', 8),\n",
       " ('decay', 8),\n",
       " ('cold', 8),\n",
       " ('sleeping', 8),\n",
       " ('daylight', 8),\n",
       " ('taken', 8),\n",
       " ('hastily', 8),\n",
       " ('glare', 8),\n",
       " ('shaft', 8),\n",
       " ('eloi', 8),\n",
       " ('south', 8),\n",
       " ('box', 8),\n",
       " ('forest', 8),\n",
       " ('law', 8),\n",
       " ('fee', 8),\n",
       " ('information', 8),\n",
       " ('anyone', 7),\n",
       " ('located', 7),\n",
       " ('country', 7),\n",
       " ('golden', 7),\n",
       " ('accepted', 7),\n",
       " ('mean', 7),\n",
       " ('anything', 7),\n",
       " ('natural', 7),\n",
       " ('dimension', 7),\n",
       " ('trace', 7),\n",
       " ('high', 7),\n",
       " ('certainly', 7),\n",
       " ('hard', 7),\n",
       " ('sure', 7),\n",
       " ('freely', 7),\n",
       " ('present', 7),\n",
       " ('become', 7),\n",
       " ('stop', 7),\n",
       " ('account', 7),\n",
       " ('attention', 7),\n",
       " ('case', 7),\n",
       " ('leave', 7),\n",
       " ('slowly', 7),\n",
       " ('held', 7),\n",
       " ('mechanism', 7),\n",
       " ('want', 7),\n",
       " ('saddle', 7),\n",
       " ('spoke', 7),\n",
       " ('laughing', 7),\n",
       " ('sounds', 7),\n",
       " ('shadows', 7),\n",
       " ('puzzled', 7),\n",
       " ('somehow', 7),\n",
       " ('paper', 7),\n",
       " ('comes', 7),\n",
       " ('mouth', 7),\n",
       " ('intense', 7),\n",
       " ('pushed', 7),\n",
       " ('seem', 7),\n",
       " ('hear', 7),\n",
       " ('rare', 7),\n",
       " ('kept', 7),\n",
       " ('rising', 7),\n",
       " ('worn', 7),\n",
       " ('shadow', 7),\n",
       " ('short', 7),\n",
       " ('ears', 7),\n",
       " ('fast', 7),\n",
       " ('splendid', 7),\n",
       " ('shivered', 7),\n",
       " ('longer', 7),\n",
       " ('absolute', 7),\n",
       " ('beyond', 7),\n",
       " ('tree', 7),\n",
       " ('touched', 7),\n",
       " ('breathing', 7),\n",
       " ('violently', 7),\n",
       " ('following', 7),\n",
       " ('fancy', 7),\n",
       " ('levers', 7),\n",
       " ('neck', 7),\n",
       " ('foot', 7),\n",
       " ('windows', 7),\n",
       " ('metal', 7),\n",
       " ('seated', 7),\n",
       " ('fruit', 7),\n",
       " ('tired', 7),\n",
       " ('river', 7),\n",
       " ('valley', 7),\n",
       " ('help', 7),\n",
       " ('living', 7),\n",
       " ('during', 7),\n",
       " ('intelligence', 7),\n",
       " ('grow', 7),\n",
       " ('pleasant', 7),\n",
       " ('problem', 7),\n",
       " ('trying', 7),\n",
       " ('narrow', 7),\n",
       " ('water', 7),\n",
       " ('protected', 7),\n",
       " ('beach', 7),\n",
       " ('returned', 7),\n",
       " ('fallen', 7),\n",
       " ('underground', 7),\n",
       " ('thousands', 7),\n",
       " ('motionless', 7),\n",
       " ('dream', 7),\n",
       " ('u', 7),\n",
       " ('distributing', 7),\n",
       " ('copies', 7),\n",
       " ('ebooks', 7),\n",
       " ('provide', 7),\n",
       " ('parts', 6),\n",
       " ('using', 6),\n",
       " ('date', 6),\n",
       " ('language', 6),\n",
       " ('start', 6),\n",
       " ('mankind', 6),\n",
       " ('speak', 6),\n",
       " ('matter', 6),\n",
       " ('silver', 6),\n",
       " ('carefully', 6),\n",
       " ('person', 6),\n",
       " ('hair', 6),\n",
       " ('existence', 6),\n",
       " ('body', 6),\n",
       " ('wait', 6),\n",
       " ('does', 6),\n",
       " ('fourth', 6),\n",
       " ('direction', 6),\n",
       " ('beginning', 6),\n",
       " ('making', 6),\n",
       " ('efforts', 6),\n",
       " ('hold', 6),\n",
       " ('words', 6),\n",
       " ('evidently', 6),\n",
       " ('getting', 6),\n",
       " ('reason', 6),\n",
       " ('vague', 6),\n",
       " ('suggested', 6),\n",
       " ('cried', 6),\n",
       " ('trick', 6),\n",
       " ('ivory', 6),\n",
       " ('unless', 6),\n",
       " ('front', 6),\n",
       " ('drew', 6),\n",
       " ('chair', 6),\n",
       " ('incredible', 6),\n",
       " ('pointed', 6),\n",
       " ('wind', 6),\n",
       " ('bare', 6),\n",
       " ('impression', 6),\n",
       " ('flickering', 6),\n",
       " ('dance', 6),\n",
       " ('showed', 6),\n",
       " ('confusion', 6),\n",
       " ('late', 6),\n",
       " ('seven', 6),\n",
       " ('quiet', 6),\n",
       " ('word', 6),\n",
       " ('warm', 6),\n",
       " ('blood', 6),\n",
       " ('read', 6),\n",
       " ('waiting', 6),\n",
       " ('hot', 6),\n",
       " ('home', 6),\n",
       " ('both', 6),\n",
       " ('started', 6),\n",
       " ('reached', 6),\n",
       " ('smoking', 6),\n",
       " ('lived', 6),\n",
       " ('thud', 6),\n",
       " ('garden', 6),\n",
       " ('afraid', 6),\n",
       " ('convey', 6),\n",
       " ('unpleasant', 6),\n",
       " ('fall', 6),\n",
       " ('twilight', 6),\n",
       " ('hillside', 6),\n",
       " ('contact', 6),\n",
       " ('possible', 6),\n",
       " ('turf', 6),\n",
       " ('smoke', 6),\n",
       " ('nearer', 6),\n",
       " ('straight', 6),\n",
       " ('reminded', 6),\n",
       " ('used', 6),\n",
       " ('sight', 6),\n",
       " ('confidence', 6),\n",
       " ('danger', 6),\n",
       " ('hitherto', 6),\n",
       " ('effort', 6),\n",
       " ('import', 6),\n",
       " ('corner', 6),\n",
       " ('watching', 6),\n",
       " ('slow', 6),\n",
       " ('discovered', 6),\n",
       " ('distance', 6),\n",
       " ('slope', 6),\n",
       " ('plants', 6),\n",
       " ('disappeared', 6),\n",
       " ('close', 6),\n",
       " ('west', 6),\n",
       " ('signs', 6),\n",
       " ('truth', 6),\n",
       " ('animals', 6),\n",
       " ('limited', 6),\n",
       " ('fate', 6),\n",
       " ('north', 6),\n",
       " ('sometimes', 6),\n",
       " ('slept', 6),\n",
       " ('panels', 6),\n",
       " ('stir', 6),\n",
       " ('mystery', 6),\n",
       " ('presence', 6),\n",
       " ('machinery', 6),\n",
       " ('theory', 6),\n",
       " ('heat', 6),\n",
       " ('species', 6),\n",
       " ('familiar', 6),\n",
       " ('distribute', 6),\n",
       " ('permission', 6),\n",
       " ('charge', 6),\n",
       " ('distribution', 6),\n",
       " ('comply', 6),\n",
       " ('paid', 6),\n",
       " ('tax', 6),\n",
       " ('convenient', 5),\n",
       " ('burnt', 5),\n",
       " ('paradox', 5),\n",
       " ('thickness', 5),\n",
       " ('having', 5),\n",
       " ('object', 5),\n",
       " ('tendency', 5),\n",
       " ('slight', 5),\n",
       " ('difference', 5),\n",
       " ('foolish', 5),\n",
       " ('particularly', 5),\n",
       " ('lips', 5),\n",
       " ('pause', 5),\n",
       " ('gently', 5),\n",
       " ('recognised', 5),\n",
       " ('smiled', 5),\n",
       " ('velocity', 5),\n",
       " ('himself', 5),\n",
       " ('wild', 5),\n",
       " ('deep', 5),\n",
       " ('wonder', 5),\n",
       " ('metallic', 5),\n",
       " ('clock', 5),\n",
       " ('tables', 5),\n",
       " ('brass', 5),\n",
       " ('low', 5),\n",
       " ('watched', 5),\n",
       " ('notice', 5),\n",
       " ('pressed', 5),\n",
       " ('pass', 5),\n",
       " ('waste', 5),\n",
       " ('changed', 5),\n",
       " ('individual', 5),\n",
       " ('breath', 5),\n",
       " ('flame', 5),\n",
       " ('ghost', 5),\n",
       " ('pipe', 5),\n",
       " ('interval', 5),\n",
       " ('thursday', 5),\n",
       " ('simple', 5),\n",
       " ('common', 5),\n",
       " ('led', 5),\n",
       " ('corridor', 5),\n",
       " ('complete', 5),\n",
       " ('perfectly', 5),\n",
       " ('holding', 5),\n",
       " ('perceived', 5),\n",
       " ('considerable', 5),\n",
       " ('ll', 5),\n",
       " ('previous', 5),\n",
       " ('spirit', 5),\n",
       " ('surprise', 5),\n",
       " ('coat', 5),\n",
       " ('doorway', 5),\n",
       " ('brighter', 5),\n",
       " ('till', 5),\n",
       " ('business', 5),\n",
       " ('easy', 5),\n",
       " ('remained', 5),\n",
       " ('startled', 5),\n",
       " ('salt', 5),\n",
       " ('displayed', 5),\n",
       " ('true', 5),\n",
       " ('writing', 5),\n",
       " ('ceased', 5),\n",
       " ('ten', 5),\n",
       " ('falling', 5),\n",
       " ('hazy', 5),\n",
       " ('sensations', 5),\n",
       " ('helpless', 5),\n",
       " ('suggestion', 5),\n",
       " ('early', 5),\n",
       " ('fair', 5),\n",
       " ('possession', 5),\n",
       " ('civilisation', 5),\n",
       " ('occurred', 5),\n",
       " ('resolved', 5),\n",
       " ('incontinently', 5),\n",
       " ('hail', 5),\n",
       " ('shape', 5),\n",
       " ('carried', 5),\n",
       " ('hour', 5),\n",
       " ('inhuman', 5),\n",
       " ('distinct', 5),\n",
       " ('wall', 5),\n",
       " ('clad', 5),\n",
       " ('rich', 5),\n",
       " ('voices', 5),\n",
       " ('heads', 5),\n",
       " ('beauty', 5),\n",
       " ('sweet', 5),\n",
       " ('ease', 5),\n",
       " ('flinging', 5),\n",
       " ('forgotten', 5),\n",
       " ('staggered', 5),\n",
       " ('knowledge', 5),\n",
       " ('naturally', 5),\n",
       " ('deserted', 5),\n",
       " ('mass', 5),\n",
       " ('generations', 5),\n",
       " ('ways', 5),\n",
       " ('heaps', 5),\n",
       " ('fruits', 5),\n",
       " ('lower', 5),\n",
       " ('nevertheless', 5),\n",
       " ('spite', 5),\n",
       " ('cattle', 5),\n",
       " ('particular', 5),\n",
       " ('later', 5),\n",
       " ('learn', 5),\n",
       " ('name', 5),\n",
       " ('cries', 5),\n",
       " ('calm', 5),\n",
       " ('crest', 5),\n",
       " ('planet', 5),\n",
       " ('masses', 5),\n",
       " ('experience', 5),\n",
       " ('secure', 5),\n",
       " ('yellow', 5),\n",
       " ('horizon', 5),\n",
       " ('steadily', 5),\n",
       " ('leaving', 5),\n",
       " ('current', 5),\n",
       " ('hither', 5),\n",
       " ('thither', 5),\n",
       " ('increasing', 5),\n",
       " ('support', 5),\n",
       " ('restless', 5),\n",
       " ('energy', 5),\n",
       " ('weak', 5),\n",
       " ('pain', 5),\n",
       " ('folly', 5),\n",
       " ('removed', 5),\n",
       " ('covered', 5),\n",
       " ('whose', 5),\n",
       " ('circumstances', 5),\n",
       " ('poor', 5),\n",
       " ('probably', 5),\n",
       " ('keep', 5),\n",
       " ('horror', 5),\n",
       " ('land', 5),\n",
       " ('path', 5),\n",
       " ('reflection', 5),\n",
       " ('safe', 5),\n",
       " ('afternoon', 5),\n",
       " ('best', 5),\n",
       " ('carry', 5),\n",
       " ('dreaded', 5),\n",
       " ('nights', 5),\n",
       " ('clambering', 5),\n",
       " ('outside', 5),\n",
       " ('fingers', 5),\n",
       " ('edge', 5),\n",
       " ('habit', 5),\n",
       " ('museum', 5),\n",
       " ('burning', 5),\n",
       " ('killing', 5),\n",
       " ('eastward', 5),\n",
       " ('grass', 5),\n",
       " ('provided', 5),\n",
       " ('phrase', 5),\n",
       " ('compliance', 5),\n",
       " ('donate', 5),\n",
       " ('medium', 5),\n",
       " ('replacement', 5),\n",
       " ('volunteers', 5),\n",
       " ('online', 4),\n",
       " ('check', 4),\n",
       " ('character', 4),\n",
       " ('trap', 4),\n",
       " ('brightly', 4),\n",
       " ('flashed', 4),\n",
       " ('geometry', 4),\n",
       " ('expect', 4),\n",
       " ('begin', 4),\n",
       " ('admit', 4),\n",
       " ('proceeded', 4),\n",
       " ('call', 4),\n",
       " ('latter', 4),\n",
       " ('cigar', 4),\n",
       " ('provincial', 4),\n",
       " ('mayor', 4),\n",
       " ('manner', 4),\n",
       " ('curious', 4),\n",
       " ('proper', 4),\n",
       " ('weather', 4),\n",
       " ('movement', 4),\n",
       " ('freedom', 4),\n",
       " ('passing', 4),\n",
       " ('miles', 4),\n",
       " ('difficulty', 4),\n",
       " ('savage', 4),\n",
       " ('show', 4),\n",
       " ('laughter', 4),\n",
       " ('money', 4),\n",
       " ('discover', 4),\n",
       " ('weary', 4),\n",
       " ('smiling', 4),\n",
       " ('passage', 4),\n",
       " ('scarcely', 4),\n",
       " ('substance', 4),\n",
       " ('legs', 4),\n",
       " ('singularly', 4),\n",
       " ('seat', 4),\n",
       " ('turning', 4),\n",
       " ('blown', 4),\n",
       " ('indistinct', 4),\n",
       " ('damned', 4),\n",
       " ('journey', 4),\n",
       " ('visible', 4),\n",
       " ('serious', 4),\n",
       " ('remarked', 4),\n",
       " ('asked', 4),\n",
       " ('plausible', 4),\n",
       " ('tomorrow', 4),\n",
       " ('broad', 4),\n",
       " ('bench', 4),\n",
       " ('sheets', 4),\n",
       " ('touch', 4),\n",
       " ('easily', 4),\n",
       " ('whom', 4),\n",
       " ('five', 4),\n",
       " ('d', 4),\n",
       " ('ve', 4),\n",
       " ('besides', 4),\n",
       " ('absence', 4),\n",
       " ('week', 4),\n",
       " ('facing', 4),\n",
       " ('smile', 4),\n",
       " ('remembered', 4),\n",
       " ('brought', 4),\n",
       " ('doing', 4),\n",
       " ('recover', 4),\n",
       " ('resumed', 4),\n",
       " ('curiosity', 4),\n",
       " ('friend', 4),\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['that',\n",
       "  'i',\n",
       "  'noticed',\n",
       "  'for',\n",
       "  'the',\n",
       "  'first',\n",
       "  'time',\n",
       "  'how',\n",
       "  'warm',\n",
       "  'the',\n",
       "  'air',\n",
       "  'was'],\n",
       " [9, 4, 518, 17, 1, 98, 19, 104, 698, 1, 199, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[666], vocab[tokens[666]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def load_corpus():\n",
    "    lines = read_data()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    return corpus, vocab\n",
    "\n",
    "\n",
    "def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n",
    "    \"\"\"使用随机抽样生成一个小批量子序列\"\"\"\n",
    "    # 从随机偏移量开始对序列进行分区，随机范围包括num_steps-1\n",
    "    import random\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    # 减去1，是因为我们需要考虑标签\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    # 长度为num_steps的子序列的起始索引\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    # 在随机抽样的迭代过程中，\n",
    "    # 来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        # 返回从pos位置开始的长度为num_steps的序列\n",
    "        return corpus[pos:pos + num_steps]\n",
    "\n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # 在这里，initial_indices包含子序列的随机起始索引\n",
    "        initial_indices_per_batch = initial_indices[i:i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield tf.constant(X), tf.constant(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, vocab = load_corpus()\n",
    "BATCH_SIZE = 100\n",
    "NUM_STEPS = 60\n",
    "train_iter = seq_data_iter_random(corpus=corpus,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  num_steps=NUM_STEPS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch[1\\200 loss: 2.796890\n",
      "epoch[2\\200 loss: 2.591876\n",
      "epoch[3\\200 loss: 2.472374\n",
      "epoch[4\\200 loss: 2.336780\n",
      "epoch[5\\200 loss: 2.297394\n",
      "epoch[6\\200 loss: 2.267097\n",
      "epoch[7\\200 loss: 2.213108\n",
      "epoch[8\\200 loss: 2.151151\n",
      "epoch[9\\200 loss: 2.151937\n",
      "epoch[10\\200 loss: 2.182924\n",
      "epoch[11\\200 loss: 2.168819\n",
      "epoch[12\\200 loss: 2.159393\n",
      "epoch[13\\200 loss: 2.135829\n",
      "epoch[14\\200 loss: 2.116175\n",
      "epoch[15\\200 loss: 2.106505\n",
      "epoch[16\\200 loss: 2.102724\n",
      "epoch[17\\200 loss: 2.091781\n",
      "epoch[18\\200 loss: 2.043946\n",
      "epoch[19\\200 loss: 2.084870\n",
      "epoch[20\\200 loss: 2.016462\n",
      "epoch[21\\200 loss: 2.015875\n",
      "epoch[22\\200 loss: 2.030103\n",
      "epoch[23\\200 loss: 1.992298\n",
      "epoch[24\\200 loss: 1.979326\n",
      "epoch[25\\200 loss: 1.975075\n",
      "epoch[26\\200 loss: 1.916403\n",
      "epoch[27\\200 loss: 1.922043\n",
      "epoch[28\\200 loss: 1.964500\n",
      "epoch[29\\200 loss: 1.904190\n",
      "epoch[30\\200 loss: 1.882347\n",
      "epoch[31\\200 loss: 1.844725\n",
      "epoch[32\\200 loss: 1.852497\n",
      "epoch[33\\200 loss: 1.840168\n",
      "epoch[34\\200 loss: 1.827344\n",
      "epoch[35\\200 loss: 1.809529\n",
      "epoch[36\\200 loss: 1.792881\n",
      "epoch[37\\200 loss: 1.789644\n",
      "epoch[38\\200 loss: 1.824399\n",
      "epoch[39\\200 loss: 1.704646\n",
      "epoch[40\\200 loss: 1.759459\n",
      "epoch[41\\200 loss: 1.707657\n",
      "epoch[42\\200 loss: 1.704166\n",
      "epoch[43\\200 loss: 1.665955\n",
      "epoch[44\\200 loss: 1.654233\n",
      "epoch[45\\200 loss: 1.646467\n",
      "epoch[46\\200 loss: 1.682441\n",
      "epoch[47\\200 loss: 1.656320\n",
      "epoch[48\\200 loss: 1.615770\n",
      "epoch[49\\200 loss: 1.648536\n",
      "epoch[50\\200 loss: 1.603304\n",
      "epoch[51\\200 loss: 1.630102\n",
      "epoch[52\\200 loss: 1.551502\n",
      "epoch[53\\200 loss: 1.552502\n",
      "epoch[54\\200 loss: 1.536734\n",
      "epoch[55\\200 loss: 1.553194\n",
      "epoch[56\\200 loss: 1.510348\n",
      "epoch[57\\200 loss: 1.506215\n",
      "epoch[58\\200 loss: 1.527253\n",
      "epoch[59\\200 loss: 1.505520\n",
      "epoch[60\\200 loss: 1.481138\n",
      "epoch[61\\200 loss: 1.499896\n",
      "epoch[62\\200 loss: 1.461424\n",
      "epoch[63\\200 loss: 1.440865\n",
      "epoch[64\\200 loss: 1.462438\n",
      "epoch[65\\200 loss: 1.463536\n",
      "epoch[66\\200 loss: 1.447071\n",
      "epoch[67\\200 loss: 1.389841\n",
      "epoch[68\\200 loss: 1.440935\n",
      "epoch[69\\200 loss: 1.367364\n",
      "epoch[70\\200 loss: 1.379046\n",
      "epoch[71\\200 loss: 1.384605\n",
      "epoch[72\\200 loss: 1.386581\n",
      "epoch[73\\200 loss: 1.375236\n",
      "epoch[74\\200 loss: 1.408860\n",
      "epoch[75\\200 loss: 1.314332\n",
      "epoch[76\\200 loss: 1.354223\n",
      "epoch[77\\200 loss: 1.308140\n",
      "epoch[78\\200 loss: 1.333795\n",
      "epoch[79\\200 loss: 1.287253\n",
      "epoch[80\\200 loss: 1.298936\n",
      "epoch[81\\200 loss: 1.246281\n",
      "epoch[82\\200 loss: 1.288962\n",
      "epoch[83\\200 loss: 1.279947\n",
      "epoch[84\\200 loss: 1.261229\n",
      "epoch[85\\200 loss: 1.289175\n",
      "epoch[86\\200 loss: 1.262099\n",
      "epoch[87\\200 loss: 1.271450\n",
      "epoch[88\\200 loss: 1.245260\n",
      "epoch[89\\200 loss: 1.218950\n",
      "epoch[90\\200 loss: 1.218250\n",
      "epoch[91\\200 loss: 1.243066\n",
      "epoch[92\\200 loss: 1.217296\n",
      "epoch[93\\200 loss: 1.209588\n",
      "epoch[94\\200 loss: 1.225147\n",
      "epoch[95\\200 loss: 1.186114\n",
      "epoch[96\\200 loss: 1.223143\n",
      "epoch[97\\200 loss: 1.181717\n",
      "epoch[98\\200 loss: 1.192593\n",
      "epoch[99\\200 loss: 1.176722\n",
      "epoch[100\\200 loss: 1.168094\n",
      "epoch[101\\200 loss: 1.192739\n",
      "epoch[102\\200 loss: 1.154889\n",
      "epoch[103\\200 loss: 1.156328\n",
      "epoch[104\\200 loss: 1.149719\n",
      "epoch[105\\200 loss: 1.151411\n",
      "epoch[106\\200 loss: 1.150404\n",
      "epoch[107\\200 loss: 1.122356\n",
      "epoch[108\\200 loss: 1.129995\n",
      "epoch[109\\200 loss: 1.125830\n",
      "epoch[110\\200 loss: 1.124260\n",
      "epoch[111\\200 loss: 1.064957\n",
      "epoch[112\\200 loss: 1.068590\n",
      "epoch[113\\200 loss: 1.112038\n",
      "epoch[114\\200 loss: 1.081649\n",
      "epoch[115\\200 loss: 1.090484\n",
      "epoch[116\\200 loss: 1.077676\n",
      "epoch[117\\200 loss: 1.049040\n",
      "epoch[118\\200 loss: 1.058683\n",
      "epoch[119\\200 loss: 1.050426\n",
      "epoch[120\\200 loss: 1.035729\n",
      "epoch[121\\200 loss: 1.040560\n",
      "epoch[122\\200 loss: 1.050748\n",
      "epoch[123\\200 loss: 1.020215\n",
      "epoch[124\\200 loss: 1.001139\n",
      "epoch[125\\200 loss: 1.014785\n",
      "epoch[126\\200 loss: 1.038132\n",
      "epoch[127\\200 loss: 1.020038\n",
      "epoch[128\\200 loss: 1.053981\n",
      "epoch[129\\200 loss: 1.017567\n",
      "epoch[130\\200 loss: 1.038432\n",
      "epoch[131\\200 loss: 1.026714\n",
      "epoch[132\\200 loss: 0.994766\n",
      "epoch[133\\200 loss: 1.007806\n",
      "epoch[134\\200 loss: 0.991661\n",
      "epoch[135\\200 loss: 0.982860\n",
      "epoch[136\\200 loss: 0.982910\n",
      "epoch[137\\200 loss: 0.989101\n",
      "epoch[138\\200 loss: 0.959415\n",
      "epoch[139\\200 loss: 1.002781\n",
      "epoch[140\\200 loss: 0.993301\n",
      "epoch[141\\200 loss: 0.951977\n",
      "epoch[142\\200 loss: 0.988727\n",
      "epoch[143\\200 loss: 0.941072\n",
      "epoch[144\\200 loss: 0.920536\n",
      "epoch[145\\200 loss: 0.957843\n",
      "epoch[146\\200 loss: 0.972696\n",
      "epoch[147\\200 loss: 0.974604\n",
      "epoch[148\\200 loss: 0.910526\n",
      "epoch[149\\200 loss: 0.966038\n",
      "epoch[150\\200 loss: 0.970182\n",
      "epoch[151\\200 loss: 0.952537\n",
      "epoch[152\\200 loss: 0.934035\n",
      "epoch[153\\200 loss: 0.936173\n",
      "epoch[154\\200 loss: 0.939172\n",
      "epoch[155\\200 loss: 0.950362\n",
      "epoch[156\\200 loss: 0.914023\n",
      "epoch[157\\200 loss: 0.899112\n",
      "epoch[158\\200 loss: 0.926848\n",
      "epoch[159\\200 loss: 0.918707\n",
      "epoch[160\\200 loss: 0.916163\n",
      "epoch[161\\200 loss: 0.927494\n",
      "epoch[162\\200 loss: 0.921890\n",
      "epoch[163\\200 loss: 0.911191\n",
      "epoch[164\\200 loss: 0.918194\n",
      "epoch[165\\200 loss: 0.865628\n",
      "epoch[166\\200 loss: 0.878681\n",
      "epoch[167\\200 loss: 0.899295\n",
      "epoch[168\\200 loss: 0.891855\n",
      "epoch[169\\200 loss: 0.902842\n",
      "epoch[170\\200 loss: 0.883385\n",
      "epoch[171\\200 loss: 0.862092\n",
      "epoch[172\\200 loss: 0.889330\n",
      "epoch[173\\200 loss: 0.869875\n",
      "epoch[174\\200 loss: 0.883124\n",
      "epoch[175\\200 loss: 0.874927\n",
      "epoch[176\\200 loss: 0.882727\n",
      "epoch[177\\200 loss: 0.881780\n",
      "epoch[178\\200 loss: 0.903693\n",
      "epoch[179\\200 loss: 0.852417\n",
      "epoch[180\\200 loss: 0.875055\n",
      "epoch[181\\200 loss: 0.845598\n",
      "epoch[182\\200 loss: 0.831684\n",
      "epoch[183\\200 loss: 0.895220\n",
      "epoch[184\\200 loss: 0.853054\n",
      "epoch[185\\200 loss: 0.840509\n",
      "epoch[186\\200 loss: 0.815560\n",
      "epoch[187\\200 loss: 0.858248\n",
      "epoch[188\\200 loss: 0.866828\n",
      "epoch[189\\200 loss: 0.854854\n",
      "epoch[190\\200 loss: 0.848924\n",
      "epoch[191\\200 loss: 0.827397\n",
      "epoch[192\\200 loss: 0.888482\n",
      "epoch[193\\200 loss: 0.830539\n",
      "epoch[194\\200 loss: 0.840031\n",
      "epoch[195\\200 loss: 0.781435\n",
      "epoch[196\\200 loss: 0.820985\n",
      "epoch[197\\200 loss: 0.841365\n",
      "epoch[198\\200 loss: 0.843009\n",
      "epoch[199\\200 loss: 0.851375\n",
      "epoch[200\\200 loss: 0.823912\n",
      "[3, 10, 2, 1, 3, 5, 13, 2, 1, 13, 4, 15, 10, 5, 7, 2, 1, 4, 7, 11, 1, 8, 14, 15, 10, 1, 6, 3, 10, 2, 9, 8, 1, 12, 5, 22, 2, 8, 1, 4, 7, 11, 1, 10, 4, 22, 2, 1, 13, 4, 7, 27, 14, 2, 2, 13, 13, 5, 3, 3, 2, 12, 3, 1, 3, 10]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'the time machine and such others lives and have manqueemmittelt th'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RNN:\n",
    "\n",
    "    def __init__(self, vocab_size, hiddens, batch_size):\n",
    "        # rnn模型输入和输出单元是一样的，都是词表的大小\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hiddens = hiddens\n",
    "        self.batch_size = batch_size\n",
    "        inputs = outputs = vocab_size\n",
    "\n",
    "        def normal(shape):\n",
    "            return tf.random.normal(shape=shape,\n",
    "                                    stddev=.01,\n",
    "                                    mean=0,\n",
    "                                    dtype='float32')\n",
    "\n",
    "        # 隐藏层参数\n",
    "        self.w_xh = tf.Variable(normal((inputs, hiddens)), dtype='float32')\n",
    "        self.w_hh = tf.Variable(normal((hiddens, hiddens)), dtype='float32')\n",
    "        self.b_h = tf.Variable(normal((hiddens, )), dtype='float32')\n",
    "        # 输出层参数\n",
    "        self.w_hq = tf.Variable(normal((hiddens, outputs)), dtype='float32')\n",
    "        self.b_q = tf.Variable(normal((outputs, )), dtype='float32')\n",
    "        self.params = [self.w_xh, self.w_hh, self.b_h, self.w_hq, self.b_q]\n",
    "\n",
    "    def init_state(self, batch_size):\n",
    "        return (tf.zeros(shape=(batch_size, self.hiddens), dtype='float32'), )\n",
    "\n",
    "    def __call__(self, x, state):\n",
    "        x = tf.one_hot(tf.transpose(x), self.vocab_size)\n",
    "        x = tf.cast(x, dtype='float32')\n",
    "        y = []\n",
    "        state, = state\n",
    "        for x_ in x:\n",
    "            x_ = tf.reshape(x_, [-1, self.w_xh.shape[0]])\n",
    "            # state = tf.nn.relu(tf.matmul(x_, self.w_xh) + tf.matmul(state, self.w_hh) + self.b_h)\n",
    "            state = tf.tanh(\n",
    "                tf.matmul(x_, self.w_xh) + tf.matmul(state, self.w_hh) +\n",
    "                self.b_h)\n",
    "            y_ = tf.matmul(state, self.w_hq) + self.b_q\n",
    "            y.append(y_)\n",
    "        return tf.concat(y, axis=0), (state, )\n",
    "\n",
    "    def predict(self, x, n_pred, vocab: Vocab):\n",
    "        pred_state = self.init_state(batch_size=1)\n",
    "        y = [vocab[x[0]]]\n",
    "        # 先预热state，其实就是记录x的隐藏状态\n",
    "        for y_ in x[1:]:\n",
    "            _, pred_state = self.__call__(\n",
    "                tf.reshape(tf.constant(y[-1]), (1, 1)).numpy(), pred_state)\n",
    "            y.append(vocab[y_])\n",
    "        # 开始利用x的隐藏状态进行预测\n",
    "        for _ in range(n_pred):\n",
    "            y_, pred_state = self.__call__(\n",
    "                tf.reshape(tf.constant(y[-1]), (1, 1)).numpy(), pred_state)\n",
    "            y.append(y_.numpy().argmax(axis=1)[0])\n",
    "        print(y)\n",
    "        return ''.join([vocab.index_to_token[c] for c in y])\n",
    "\n",
    "    def gradient_clip(self, grads, theta):\n",
    "        \"\"\"\n",
    "            梯度裁剪，防止梯度爆炸问题\n",
    "        \"\"\"\n",
    "        theta = tf.constant(theta, dtype=\"float32\")\n",
    "        new_grads = []\n",
    "        for grad in grads:\n",
    "            new_grads.append(\n",
    "                tf.convert_to_tensor(grad) if isinstance(\n",
    "                    grad, tf.IndexedSlices) else grad)\n",
    "        # L2范数\n",
    "        norm = tf.math.sqrt(\n",
    "            sum((tf.reduce_sum(grad**2).numpy() for grad in new_grads)))\n",
    "        norm = tf.cast(norm, \"float32\")\n",
    "        if tf.greater(norm, theta):\n",
    "            for i, grad in enumerate(new_grads):\n",
    "                new_grads[i] = grad * theta / norm\n",
    "        return new_grads\n",
    "    \n",
    "    def fit(self, train_iter, strategy, epochs=10, lr=1e-3):\n",
    "        with strategy.scope():\n",
    "            loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "            optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "        for epoch in range(epochs):\n",
    "            for x, y in train_iter():\n",
    "                state = self.init_state(x.shape[0])\n",
    "                with tf.GradientTape(persistent=True) as gt:\n",
    "                    # 向前计算\n",
    "                    y_hat, state = self.__call__(x, state)\n",
    "                    # y_hat = tf.nn.softmax(y_hat)\n",
    "                    y = tf.reshape(tf.transpose(y), (-1))\n",
    "                    # 损失计算\n",
    "                    l = loss_fn(y, y_hat)\n",
    "                grads = gt.gradient(l, self.params)\n",
    "                grads = self.gradient_clip(grads, 1)\n",
    "                # print(tf.reduce_sum(grads[0]))\n",
    "                optimizer.apply_gradients(zip(grads, self.params))\n",
    "            print(\"epoch[%d\\%d loss: %f\" % (epoch + 1, epochs, tf.reduce_mean(l)))\n",
    "\n",
    "\n",
    "strategy = tf.distribute.OneDeviceStrategy('/gpu:0')\n",
    "with strategy.scope():\n",
    "    net = RNN(len(vocab), 512, BATCH_SIZE)\n",
    "def get_train_iter():\n",
    "    return seq_data_iter_random(corpus=corpus,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  num_steps=NUM_STEPS)\n",
    "net.fit(train_iter=get_train_iter, strategy=strategy, epochs=200, lr=1e-3)\n",
    "net.predict('the time machine', 50, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 19, 4, 7, 18, 12, 2, 4, 3, 5, 13, 2, 1, 3, 5, 13, 2, 1, 5, 7, 1, 3, 10, 2, 1, 4, 7, 11, 1, 3, 6, 1, 12, 2, 4, 9, 7, 1, 6, 22, 2, 9, 1, 12, 4, 7, 18, 2, 9, 1, 17, 4, 8, 1, 5, 12, 12, 1, 15, 6, 7, 3, 2, 5, 7, 3, 12, 19, 1, 3, 10, 2, 1, 2, 4, 9, 3, 10, 1, 8, 2, 2, 1, 5, 7, 1, 4, 1, 20, 4, 5, 7, 1, 6, 7, 1, 3, 10, 2, 1, 2, 22, 2, 7, 1, 7, 6, 17, 17, 10, 4, 3, 1, 3, 10, 2, 1, 3, 10, 5, 7, 18, 1, 5, 1, 10, 4, 22, 2, 1, 8, 4, 5, 11, 1, 10, 4, 22, 2, 1, 3, 10, 2, 13, 1, 5, 7, 1, 5, 3, 8, 3, 9, 4, 7, 18, 2, 1, 21, 2, 2, 7, 4, 7, 1, 12, 5, 3, 3, 12, 2, 1, 3, 5, 13, 2, 1, 5, 1, 8, 4, 17, 1, 3, 10, 2, 13, 2, 11, 11, 5, 7, 18, 1, 4, 7, 11, 1, 3, 10, 2, 1, 3, 10, 5, 7, 18, 1, 17, 4, 8, 1, 7, 6, 3, 1, 14, 7, 12, 5, 23, 2, 1, 4, 1, 27, 14, 2, 8, 3, 5, 6, 7, 1, 4, 11, 6, 7, 1, 3, 10, 2, 1, 6, 3, 10, 2, 9, 1, 3, 10, 2, 1, 17, 6, 9, 23, 1, 5, 7, 1, 4, 7, 19, 1, 5, 13, 20, 9, 2, 8, 8, 5, 6, 7, 1, 6, 16, 1, 13, 4, 7, 19, 2, 12, 11, 6, 8, 3, 1, 20, 6, 9, 14, 8, 3, 1, 17, 4, 8, 1, 16, 12, 4, 13, 2, 1, 5, 7, 1, 3, 10, 2, 1, 11, 6, 6, 9, 17, 4, 19, 1, 4, 8, 1, 5, 16, 1, 10, 2, 1, 10, 4, 11, 1, 21, 2, 2, 7, 11, 4, 26, 26, 12, 2, 11, 1, 21, 19, 1, 3, 10, 2, 1, 12, 5, 18, 10, 3, 1, 3, 10, 2, 7, 1, 10, 2, 1, 15, 6, 13, 5, 7, 18, 1, 8, 4, 20, 2, 9, 3, 4, 9, 2, 1, 17, 4, 8, 1, 3, 10, 2, 1, 6, 7, 12, 19, 1, 3, 2, 2, 11, 16, 14, 3, 1, 4, 7, 11, 1, 3, 10, 2, 1, 8, 4, 13, 2, 1, 20, 5, 8, 10, 2, 7, 3, 1, 3, 6, 1, 20, 9, 2, 3, 4, 19, 11, 5, 7, 4, 20, 2, 9, 16, 2, 15, 3, 5, 6, 7, 1, 4, 21, 8, 14, 12, 11, 1, 3, 10, 5, 8, 1, 20, 2, 9, 16, 2, 15, 3, 12, 19, 1, 8, 4, 7, 11, 1, 3, 10, 2, 9, 2, 1, 17, 2, 9, 2, 1, 7, 6, 1, 8, 4, 15, 23, 1, 3, 6, 1, 17, 10, 5, 3, 2, 1, 13, 4, 9, 23, 5, 7, 18, 1, 4, 8, 1, 3, 10, 2, 1, 13, 6, 9, 12, 6, 15, 23, 8, 1, 20, 6, 6, 23, 2, 11, 1, 4, 18, 4, 5, 7, 8, 3, 1, 3, 10, 2, 1, 10, 4, 5, 12, 1, 5, 7, 1, 4, 1, 3, 14, 9, 7, 2, 11, 1, 6, 7, 12, 1, 3, 10, 2, 1, 18, 4, 12, 12, 2, 9, 19, 1, 17, 4, 8, 1, 5, 7, 1, 16, 4, 15, 3, 1, 4, 7, 1, 3, 10, 2, 1, 3, 10, 9, 2, 2, 1, 20, 12, 4, 15, 2, 1, 10, 4, 11, 1, 21, 2, 2, 7, 4, 8, 8, 4, 3, 5, 8, 2, 11, 1, 13, 2, 11, 5, 15, 4, 12, 1, 13, 4, 7, 1, 4, 9, 2, 1, 19, 6, 14, 1, 20, 4, 5, 7, 1, 4, 7, 11, 1, 3, 10, 2, 1, 3, 10, 5, 7, 18, 1, 4, 8, 1, 3, 6, 1, 14, 7, 11, 2, 9, 8, 3, 4, 7, 11, 1, 3, 10, 2, 1, 18, 4, 12, 12, 2, 9, 19, 1, 17, 4, 8, 1, 3, 10, 2, 1, 7, 2, 17, 1, 8, 6, 7, 8, 1, 18, 6, 7, 11, 1, 3, 10, 2, 1, 16, 14, 12, 12, 1, 20, 9, 6, 25, 2, 15, 3, 1, 18, 14, 3, 2, 7, 21, 2, 9, 18, 1, 3, 13, 1, 17, 6, 9, 23, 1, 4, 7, 11, 1, 15, 9, 2, 4, 3, 14, 9, 2, 1, 20, 2, 9, 10, 4, 20, 8, 1, 4, 1, 12, 5, 3, 3, 12, 2, 1, 8, 3, 6, 20, 20, 5, 7, 18, 1, 17, 4, 8, 1, 21, 6, 16, 2, 12, 19, 1, 4, 3, 9, 5, 15, 3, 12, 19, 1, 6, 16, 1, 3, 10, 2, 1, 11, 5, 8, 3, 4, 7, 15, 2, 1, 16, 9, 2, 2, 11, 6, 11, 1, 8, 6, 14, 7, 11, 8, 1, 3, 6, 1, 8, 10, 2, 1, 17, 2, 12, 12, 1, 6, 16, 1, 4, 7, 1, 4, 7, 2, 13, 6, 3, 5, 6, 7, 1, 4, 7, 11, 1, 3, 10, 2, 1, 16, 5, 9, 5, 7, 18, 1, 20, 9, 6, 25, 2, 15, 3, 18, 14, 3, 2, 7, 21, 2, 9, 18, 1, 3, 13, 1, 2, 12, 2, 15, 3, 9, 6, 7, 5, 15, 1, 17, 6, 9, 23, 8, 1, 5, 7, 1, 19, 6, 14, 9, 20, 6, 8, 8, 2, 8, 8, 5, 6, 7, 1, 6, 16, 1, 13, 2, 1, 4, 7, 11, 1, 8, 6, 1, 5, 1, 7, 6, 3, 5, 15, 2, 11, 1, 3, 10, 4, 3, 1, 3, 10, 2, 1, 20, 9, 6, 25, 2, 15, 3, 18, 14, 3, 2, 7, 21, 2, 9, 18, 1, 12, 5, 3, 2, 9, 4, 9, 19, 1, 4, 9, 15, 10, 5, 22, 2, 1, 16, 6, 14, 7, 11, 4, 3, 5, 6, 7, 1, 3, 10, 2, 1, 6, 20, 10, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fyangleatime time in the and to learn over langer was ill conteintly the earth see in a pain on the even nowwhat the thing i have said have them in itstrange beenan little time i saw themedding and the thing was not unlike a question adon the other the work in any impression of manyeldost porust was flame in the doorway as if he had beendazzled by the light then he coming sapertare was the only teedfut and the same pishent to pretaydinaperfection absuld this perfectly sand there were no sack to white marking as the morlocks pooked against the hail in a turned onl the gallery was in fact an the three place had beenassatised medical man are you pain and the thing as to understand the gallery was the new sons gond the full project gutenberg tm work and creature perhaps a little stopping was bofely atrictly of the distance freedod sounds to she well of an anemotion and the firing projectgutenberg tm electronic works in yourpossession of me and so i noticed that the projectgutenberg literary archive foundation the ophe'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.predict(\"fyang\", 1024, vocab)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ae33f7c48cc3e1271596d1bf08ce4d5e6d6f7129ff8bbb83bbb95ed8addff62"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
