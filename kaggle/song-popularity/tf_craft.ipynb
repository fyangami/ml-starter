{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(\"song_data.csv\").to_numpy()\n",
    "song_names = data[:, 0]\n",
    "song_labels = data[:, 1].reshape(len(data), 1).astype('float64')\n",
    "song_data = data[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from utils.preprocess import std\n",
    "from utils.preprocess import split_train_test\n",
    "# song_data = std(song_data)\n",
    "train_data, train_label, test_data, test_label = split_train_test(song_data, song_labels)\n",
    "train_data, test_data = std(train_data), std(test_data)\n",
    "train_data, train_label, test_data, test_label = tf.convert_to_tensor(train_data, tf.float32), tf.convert_to_tensor(train_label, tf.float32), tf.convert_to_tensor(test_data,tf.float32), tf.convert_to_tensor(test_label, tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据生成器\n",
    "def data_generator(train_data, train_label, batch_size=20):\n",
    "    n_ = len(train_data)\n",
    "    indicates = list(range(n_))\n",
    "    tf.random.shuffle(indicates)\n",
    "    for batch in range(0, n_, batch_size):\n",
    "        chose = tf.constant(indicates[batch: min(batch + batch_size, n_)])\n",
    "        yield tf.gather(train_data, chose), tf.gather(train_label, chose)\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "def liner_regression(X, w, b):\n",
    "    return tf.matmul(X, w) + b\n",
    "\n",
    "# 定义损失函数，均值方差\n",
    "def loss(y_hat, y):\n",
    "    return (y_hat - y) ** 2 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, test loss: 110.7847900390625\n",
      "epoch: 2, test loss: 53.635128021240234\n",
      "epoch: 3, test loss: 26.4747257232666\n",
      "epoch: 4, test loss: 13.34717845916748\n",
      "epoch: 5, test loss: 6.894477844238281\n",
      "epoch: 6, test loss: 3.665008544921875\n",
      "epoch: 7, test loss: 2.016911029815674\n",
      "epoch: 8, test loss: 1.158096194267273\n",
      "epoch: 9, test loss: 0.7005805969238281\n",
      "epoch: 10, test loss: 0.45122265815734863\n",
      "epoch: 11, test loss: 0.3120880722999573\n",
      "epoch: 12, test loss: 0.2325950264930725\n",
      "epoch: 13, test loss: 0.1860940158367157\n",
      "epoch: 14, test loss: 0.1582423448562622\n",
      "epoch: 15, test loss: 0.14116784930229187\n",
      "epoch: 16, test loss: 0.13045097887516022\n",
      "epoch: 17, test loss: 0.12356610596179962\n",
      "epoch: 18, test loss: 0.11903305351734161\n",
      "epoch: 19, test loss: 0.11597520112991333\n",
      "epoch: 20, test loss: 0.11386191844940186\n",
      "epoch: 21, test loss: 0.1123591735959053\n",
      "epoch: 22, test loss: 0.11126600205898285\n",
      "epoch: 23, test loss: 0.11044394969940186\n",
      "epoch: 24, test loss: 0.10981246083974838\n",
      "epoch: 25, test loss: 0.10931294411420822\n",
      "epoch: 26, test loss: 0.10891130566596985\n",
      "epoch: 27, test loss: 0.10857566446065903\n",
      "epoch: 28, test loss: 0.10829444229602814\n",
      "epoch: 29, test loss: 0.10805190354585648\n",
      "epoch: 30, test loss: 0.107843317091465\n",
      "epoch: 31, test loss: 0.10765787959098816\n",
      "epoch: 32, test loss: 0.10748989135026932\n",
      "epoch: 33, test loss: 0.1073414608836174\n",
      "epoch: 34, test loss: 0.10720710456371307\n",
      "epoch: 35, test loss: 0.10708137601613998\n",
      "epoch: 36, test loss: 0.1069684699177742\n",
      "epoch: 37, test loss: 0.10686355084180832\n",
      "epoch: 38, test loss: 0.10676421970129013\n",
      "epoch: 39, test loss: 0.10667501389980316\n",
      "epoch: 40, test loss: 0.10659010708332062\n",
      "epoch: 41, test loss: 0.10651222616434097\n",
      "epoch: 42, test loss: 0.10643450170755386\n",
      "epoch: 43, test loss: 0.10636507719755173\n",
      "epoch: 44, test loss: 0.1062985360622406\n",
      "epoch: 45, test loss: 0.10623427480459213\n",
      "epoch: 46, test loss: 0.10617433488368988\n",
      "epoch: 47, test loss: 0.10611822456121445\n",
      "epoch: 48, test loss: 0.10606400668621063\n",
      "epoch: 49, test loss: 0.10601349920034409\n",
      "epoch: 50, test loss: 0.10596464574337006\n",
      "epoch: 51, test loss: 0.10591733455657959\n",
      "epoch: 52, test loss: 0.10587512701749802\n",
      "epoch: 53, test loss: 0.10583234578371048\n",
      "epoch: 54, test loss: 0.1057908833026886\n",
      "epoch: 55, test loss: 0.10575249046087265\n",
      "epoch: 56, test loss: 0.10571702569723129\n",
      "epoch: 57, test loss: 0.10567737370729446\n",
      "epoch: 58, test loss: 0.10564393550157547\n",
      "epoch: 59, test loss: 0.10561159253120422\n",
      "epoch: 60, test loss: 0.10557999461889267\n",
      "epoch: 61, test loss: 0.10555123537778854\n",
      "epoch: 62, test loss: 0.10551793873310089\n",
      "epoch: 63, test loss: 0.10549242049455643\n",
      "epoch: 64, test loss: 0.10546600818634033\n",
      "epoch: 65, test loss: 0.10544022172689438\n",
      "epoch: 66, test loss: 0.10541343688964844\n",
      "epoch: 67, test loss: 0.10538909584283829\n",
      "epoch: 68, test loss: 0.10536714643239975\n",
      "epoch: 69, test loss: 0.10534568130970001\n",
      "epoch: 70, test loss: 0.10532137751579285\n",
      "epoch: 71, test loss: 0.10530096292495728\n",
      "epoch: 72, test loss: 0.10528131574392319\n",
      "epoch: 73, test loss: 0.10526366531848907\n",
      "epoch: 74, test loss: 0.10523971915245056\n",
      "epoch: 75, test loss: 0.10522319376468658\n",
      "epoch: 76, test loss: 0.10520710796117783\n",
      "epoch: 77, test loss: 0.10518962889909744\n",
      "epoch: 78, test loss: 0.10517428070306778\n",
      "epoch: 79, test loss: 0.10515939444303513\n",
      "epoch: 80, test loss: 0.105143241584301\n",
      "epoch: 81, test loss: 0.10512906312942505\n",
      "epoch: 82, test loss: 0.10511188209056854\n",
      "epoch: 83, test loss: 0.10510005056858063\n",
      "epoch: 84, test loss: 0.10508524626493454\n",
      "epoch: 85, test loss: 0.105075903236866\n",
      "epoch: 86, test loss: 0.10506314784288406\n",
      "epoch: 87, test loss: 0.10504922270774841\n",
      "epoch: 88, test loss: 0.10503901541233063\n",
      "epoch: 89, test loss: 0.10502727329730988\n",
      "epoch: 90, test loss: 0.10501395910978317\n",
      "epoch: 91, test loss: 0.10500458627939224\n",
      "epoch: 92, test loss: 0.10499531030654907\n",
      "epoch: 93, test loss: 0.10498446226119995\n",
      "epoch: 94, test loss: 0.10497565567493439\n",
      "epoch: 95, test loss: 0.10496537387371063\n",
      "epoch: 96, test loss: 0.10495705157518387\n",
      "epoch: 97, test loss: 0.1049487441778183\n",
      "epoch: 98, test loss: 0.10494062304496765\n",
      "epoch: 99, test loss: 0.10493103414773941\n",
      "epoch: 100, test loss: 0.10492508858442307\n",
      "epoch: 101, test loss: 0.10491404682397842\n",
      "epoch: 102, test loss: 0.10490655899047852\n",
      "epoch: 103, test loss: 0.10489778220653534\n",
      "epoch: 104, test loss: 0.10489237308502197\n",
      "epoch: 105, test loss: 0.10488545149564743\n",
      "epoch: 106, test loss: 0.1048785150051117\n",
      "epoch: 107, test loss: 0.1048734113574028\n",
      "epoch: 108, test loss: 0.10486873239278793\n",
      "epoch: 109, test loss: 0.10485880821943283\n",
      "epoch: 110, test loss: 0.10485414415597916\n",
      "epoch: 111, test loss: 0.10484964400529861\n",
      "epoch: 112, test loss: 0.10484173148870468\n",
      "epoch: 113, test loss: 0.10483574122190475\n",
      "epoch: 114, test loss: 0.1048334613442421\n",
      "epoch: 115, test loss: 0.10482576489448547\n",
      "epoch: 116, test loss: 0.10482184588909149\n",
      "epoch: 117, test loss: 0.10481783747673035\n",
      "epoch: 118, test loss: 0.10481217503547668\n",
      "epoch: 119, test loss: 0.10481022298336029\n",
      "epoch: 120, test loss: 0.10480479896068573\n",
      "epoch: 121, test loss: 0.10479959845542908\n",
      "epoch: 122, test loss: 0.10479762405157089\n",
      "epoch: 123, test loss: 0.10479410737752914\n",
      "epoch: 124, test loss: 0.10479072481393814\n",
      "epoch: 125, test loss: 0.10478394478559494\n",
      "epoch: 126, test loss: 0.10477884858846664\n",
      "epoch: 127, test loss: 0.10477732121944427\n",
      "epoch: 128, test loss: 0.10477431863546371\n",
      "epoch: 129, test loss: 0.1047692820429802\n",
      "epoch: 130, test loss: 0.10476631671190262\n",
      "epoch: 131, test loss: 0.10476330667734146\n",
      "epoch: 132, test loss: 0.10476032644510269\n",
      "epoch: 133, test loss: 0.10475556552410126\n",
      "epoch: 134, test loss: 0.10475447028875351\n",
      "epoch: 135, test loss: 0.10475163906812668\n",
      "epoch: 136, test loss: 0.10474873334169388\n",
      "epoch: 137, test loss: 0.10474613308906555\n",
      "epoch: 138, test loss: 0.10474169254302979\n",
      "epoch: 139, test loss: 0.10473724454641342\n",
      "epoch: 140, test loss: 0.10473617911338806\n",
      "epoch: 141, test loss: 0.1047353595495224\n",
      "epoch: 142, test loss: 0.10473449528217316\n",
      "epoch: 143, test loss: 0.10473207384347916\n",
      "epoch: 144, test loss: 0.10472763329744339\n",
      "epoch: 145, test loss: 0.10472529381513596\n",
      "epoch: 146, test loss: 0.10472265630960464\n",
      "epoch: 147, test loss: 0.10471852123737335\n",
      "epoch: 148, test loss: 0.10471779108047485\n",
      "epoch: 149, test loss: 0.10471539199352264\n",
      "epoch: 150, test loss: 0.10471309721469879\n",
      "epoch: 151, test loss: 0.10471250861883163\n",
      "epoch: 152, test loss: 0.10471198707818985\n",
      "epoch: 153, test loss: 0.10470971465110779\n",
      "epoch: 154, test loss: 0.10470914840698242\n",
      "epoch: 155, test loss: 0.10470511019229889\n",
      "epoch: 156, test loss: 0.10470271110534668\n",
      "epoch: 157, test loss: 0.10470230877399445\n",
      "epoch: 158, test loss: 0.10470184683799744\n",
      "epoch: 159, test loss: 0.10469967871904373\n",
      "epoch: 160, test loss: 0.10469748079776764\n",
      "epoch: 161, test loss: 0.10469706356525421\n",
      "epoch: 162, test loss: 0.10469479858875275\n",
      "epoch: 163, test loss: 0.10469457507133484\n",
      "epoch: 164, test loss: 0.1046924740076065\n",
      "epoch: 165, test loss: 0.10469024628400803\n",
      "epoch: 166, test loss: 0.1046917662024498\n",
      "epoch: 167, test loss: 0.10468962043523788\n",
      "epoch: 168, test loss: 0.10468750447034836\n",
      "epoch: 169, test loss: 0.10468529909849167\n",
      "epoch: 170, test loss: 0.10468512028455734\n",
      "epoch: 171, test loss: 0.10468485206365585\n",
      "epoch: 172, test loss: 0.1046827957034111\n",
      "epoch: 173, test loss: 0.10468070954084396\n",
      "epoch: 174, test loss: 0.10468051582574844\n",
      "epoch: 175, test loss: 0.10468026250600815\n",
      "epoch: 176, test loss: 0.10467828065156937\n",
      "epoch: 177, test loss: 0.10467813163995743\n",
      "epoch: 178, test loss: 0.10467620193958282\n",
      "epoch: 179, test loss: 0.10467402637004852\n",
      "epoch: 180, test loss: 0.10467217862606049\n",
      "epoch: 181, test loss: 0.10467202216386795\n",
      "epoch: 182, test loss: 0.10467352718114853\n",
      "epoch: 183, test loss: 0.10467321425676346\n",
      "epoch: 184, test loss: 0.1046714037656784\n",
      "epoch: 185, test loss: 0.10466950386762619\n",
      "epoch: 186, test loss: 0.104667529463768\n",
      "epoch: 187, test loss: 0.10466737300157547\n",
      "epoch: 188, test loss: 0.1046673059463501\n",
      "epoch: 189, test loss: 0.104667067527771\n",
      "epoch: 190, test loss: 0.10466690361499786\n",
      "epoch: 191, test loss: 0.10466673970222473\n",
      "epoch: 192, test loss: 0.10466305911540985\n",
      "epoch: 193, test loss: 0.1046629473567009\n",
      "epoch: 194, test loss: 0.1046629399061203\n",
      "epoch: 195, test loss: 0.10466273874044418\n",
      "epoch: 196, test loss: 0.10466252267360687\n",
      "epoch: 197, test loss: 0.10466238111257553\n",
      "epoch: 198, test loss: 0.10466235876083374\n",
      "epoch: 199, test loss: 0.10466058552265167\n",
      "epoch: 200, test loss: 0.10466038435697556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4af82cf610>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 开始训练\n",
    "epochs = 200\n",
    "eta = 1e-1\n",
    "batch_size = 200\n",
    "w = tf.Variable(tf.random.normal(shape=(train_data.shape[1], 1), mean=0, stddev=.01), trainable=True)\n",
    "b = tf.Variable(tf.zeros(1), trainable=True)\n",
    "losses = np.zeros(epochs)\n",
    "for epoch in range(epochs):\n",
    "    for X, y in data_generator(train_data, train_label, batch_size):\n",
    "        # 自动微分\n",
    "        with tf.GradientTape() as g:\n",
    "            y_hat = liner_regression(X, w, b)\n",
    "            l = loss(y_hat, y)\n",
    "        # 计算梯度\n",
    "        gw, gb = g.gradient(l, [w, b])\n",
    "        # 更新参数\n",
    "        w.assign_sub(eta * gw / batch_size)\n",
    "        b.assign_sub(eta * gb / batch_size)\n",
    "    l = loss(liner_regression(test_data, w, b), test_label)\n",
    "    losses[epoch] = tf.reduce_mean(l)\n",
    "    print(f'epoch: {epoch + 1}, test loss: {tf.reduce_mean(l)}')\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       " array([[60.54294 ],\n",
       "        [65.54213 ],\n",
       "        [66.5447  ],\n",
       "        [31.540136],\n",
       "        [45.54476 ],\n",
       "        [25.546875],\n",
       "        [28.541712],\n",
       "        [75.54021 ],\n",
       "        [53.5397  ],\n",
       "        [55.542458]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
       " array([[61.],\n",
       "        [66.],\n",
       "        [67.],\n",
       "        [32.],\n",
       "        [46.],\n",
       "        [26.],\n",
       "        [29.],\n",
       "        [76.],\n",
       "        [54.],\n",
       "        [56.]], dtype=float32)>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liner_regression(test_data[:10], w, b), test_label[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'Variable:0' shape=(14, 1) dtype=float32, numpy=\n",
       " array([[ 9.9999405e+01],\n",
       "        [-2.7675631e-02],\n",
       "        [-8.0343464e-04],\n",
       "        [-3.9976137e-03],\n",
       "        [-9.1706300e-03],\n",
       "        [ 2.5522949e-03],\n",
       "        [ 4.6974495e-05],\n",
       "        [ 2.6197862e-04],\n",
       "        [ 2.4553299e-02],\n",
       "        [-3.8399747e-05],\n",
       "        [ 2.0770893e-04],\n",
       "        [-8.4634539e-04],\n",
       "        [ 5.9826924e-03],\n",
       "        [ 1.4564444e-03]], dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([52.900337], dtype=float32)>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w, b"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "240684a6bf55e82a0f5995d45026058ae310044e71522ea1b595ad868521a9f2"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
