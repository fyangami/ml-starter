{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.datasets.mnist import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, y), (tx, ty) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, tx = x / 255., tx / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, tx = x.reshape(-1, 28 * 28), tx.reshape(-1, 28 * 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, ty = y.reshape(-1, 1), ty.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 784), (10000, 784), (60000, 1), (10000, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, tx.shape, y.shape, ty.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型常量 \n",
    "n_featrues = 28 * 28   # 每个图片有 28 * 28 个像素，转成一维的特征数量\n",
    "n_class = 10    # 总共0-9个数字，共计10个label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "\n",
    "# 前向计算\n",
    "def softmax_regression(x, w, b):\n",
    "    # x: 256 x 784  w: 784 x 10\n",
    "    # 输出是 256 x 10 的矩阵\n",
    "    # 每一行有10和元素，分别代表模型输出0-9的概率\n",
    "    logits = np.dot(x, w.T) + b\n",
    "    exp = np.exp(logits)\n",
    "    return  exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "# 精度计算函数，计算预测函数的精准度\n",
    "def accuracy(y_hat, y):\n",
    "    # 265 x 10\n",
    "    y_hat = np.argmax(y_hat, axis=1, keepdims=True).astype(y.dtype)\n",
    "    return (y_hat == y).sum() / len(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1004"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试softmax\n",
    "test_softmax_w = np.random.randn(n_class, n_featrues) # 模型共 784 x 10 个 权重\n",
    "test_softmax_b = np.random.randn(1, n_class) # 共 1 x 10个偏置\n",
    "# 理论上随便预测一次，模型大概有10%的精度\n",
    "test_softmax_y_hat = softmax_regression(tx, test_softmax_w, test_softmax_b)\n",
    "# 计算精度\n",
    "accuracy(test_softmax_y_hat, ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot编码\n",
    "def one_hot(y, n_):\n",
    "    # 256 x 1\n",
    "    assert y.shape[1] == 1\n",
    "    yy = None\n",
    "    for uni in range(n_):\n",
    "        if yy is None:\n",
    "            yy = y == uni\n",
    "        else:\n",
    "            yy = np.c_[yy, y == uni]\n",
    "    return yy.astype('uint8')\n",
    "\n",
    "#测试one_hot编码\n",
    "(np.argmax(one_hot(ty, n_=n_class), axis=1) == ty.T[0]).sum() == len(ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.919749171071098"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义损失函数---交叉熵\n",
    "def cross_entropy(y_hat, y):\n",
    "    # 处理一下y_hat，防止数值上溢和下溢\n",
    "    y_hat = np.clip(y_hat, 1e-9, 1.)\n",
    "    # y进行one-hot编码\n",
    "    y = one_hot(y, n_=n_class)\n",
    "    # 对y进行one-hot编码之后，非真值列y = 0，不参与计算 即: 1{y_i = j} else 0{y_i != j}\n",
    "    return -np.mean(np.sum(y * np.log(y_hat), axis=1))\n",
    "\n",
    "#测试损失计算\n",
    "cross_entropy(test_softmax_y_hat, ty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0., -0., -0., ..., -0., -0., -0.],\n",
       "        [-0., -0., -0., ..., -0., -0., -0.],\n",
       "        [-0., -0., -0., ..., -0., -0., -0.],\n",
       "        ...,\n",
       "        [-0., -0., -0., ..., -0., -0., -0.],\n",
       "        [-0., -0., -0., ..., -0., -0., -0.],\n",
       "        [-0., -0., -0., ..., -0., -0., -0.]]),\n",
       " -0.0898187871206671)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 梯度计算，反向传播\n",
    "def gradients(y_hat, y, x):\n",
    "    # 注意，只有y_i = j 才参与计算\n",
    "    # y进行one-hot编码\n",
    "    y = one_hot(y, n_=n_class)\n",
    "    # y: 256 x 10   x: 256 x 784\n",
    "    # 先计算梯度内部表达式\n",
    "    # 过滤掉通过*y过滤掉y = 0 的情况，此时y_j.shape: 265 x 1\n",
    "    y_j = np.sum((y - y_hat), axis=1, keepdims=True)\n",
    "    # w和b的梯度分开计算\n",
    "    # 在原有公式上直接乘以y可以消除y != 1的数值，\n",
    "    # 因为我们的y进行了one-hot编码，只有对应的位置元素为1，其余都为0\n",
    "    return -((y - y_hat) * y).T.dot(x), -np.mean((y - y_hat) * y)\n",
    "\n",
    "# 测试计算梯度\n",
    "gradients(test_softmax_y_hat, ty, tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter[0, loss: 11.492013268349108, acc: 0.12626666666666667\n",
      "iter[1, loss: 10.749944114915436, acc: 0.13391666666666666\n",
      "iter[2, loss: 10.040025392983953, acc: 0.14461666666666667\n",
      "iter[3, loss: 9.381399693241073, acc: 0.15705\n",
      "iter[4, loss: 8.782411371849244, acc: 0.16991666666666666\n",
      "iter[5, loss: 8.241672269580562, acc: 0.18311666666666668\n",
      "iter[6, loss: 7.75479109970336, acc: 0.1978\n",
      "iter[7, loss: 7.318364922720481, acc: 0.21271666666666667\n",
      "iter[8, loss: 6.928432662703104, acc: 0.2276\n",
      "iter[9, loss: 6.5802741118942185, acc: 0.24418333333333334\n",
      "iter[10, loss: 6.269860285963929, acc: 0.2597\n",
      "iter[11, loss: 5.991767733145936, acc: 0.27513333333333334\n",
      "iter[12, loss: 5.741157497255152, acc: 0.29001666666666664\n",
      "iter[13, loss: 5.514126910985687, acc: 0.30585\n",
      "iter[14, loss: 5.307547321997527, acc: 0.32125\n",
      "iter[15, loss: 5.118621322068707, acc: 0.33541666666666664\n",
      "iter[16, loss: 4.945057263292816, acc: 0.34958333333333336\n",
      "iter[17, loss: 4.785120806773232, acc: 0.36346666666666666\n",
      "iter[18, loss: 4.637474989563185, acc: 0.37585\n",
      "iter[19, loss: 4.501035161027487, acc: 0.3877833333333333\n",
      "iter[20, loss: 4.374740225356299, acc: 0.39948333333333336\n",
      "iter[21, loss: 4.257782696503206, acc: 0.41091666666666665\n",
      "iter[22, loss: 4.149399932884223, acc: 0.42151666666666665\n",
      "iter[23, loss: 4.048856049622551, acc: 0.43178333333333335\n",
      "iter[24, loss: 3.955460923150635, acc: 0.44216666666666665\n",
      "iter[25, loss: 3.868526844501503, acc: 0.45053333333333334\n",
      "iter[26, loss: 3.7873896052849076, acc: 0.45965\n",
      "iter[27, loss: 3.711477022912551, acc: 0.46785\n",
      "iter[28, loss: 3.6403339060044906, acc: 0.4760666666666667\n",
      "iter[29, loss: 3.573464510455345, acc: 0.48383333333333334\n",
      "iter[30, loss: 3.51047906614413, acc: 0.49178333333333335\n",
      "iter[31, loss: 3.451070956350052, acc: 0.4989166666666667\n",
      "iter[32, loss: 3.394947338038175, acc: 0.5065833333333334\n",
      "iter[33, loss: 3.3418353286670324, acc: 0.5135166666666666\n",
      "iter[34, loss: 3.291490164605841, acc: 0.5204166666666666\n",
      "iter[35, loss: 3.243711943495042, acc: 0.52705\n",
      "iter[36, loss: 3.1982886253837712, acc: 0.5327\n",
      "iter[37, loss: 3.1550532294617075, acc: 0.5390166666666667\n",
      "iter[38, loss: 3.113856091437763, acc: 0.5444166666666667\n",
      "iter[39, loss: 3.0745587783539783, acc: 0.5499333333333334\n",
      "iter[40, loss: 3.037010821190342, acc: 0.5547166666666666\n",
      "iter[41, loss: 3.0010982063555516, acc: 0.55975\n",
      "iter[42, loss: 2.96670372495264, acc: 0.5652333333333334\n",
      "iter[43, loss: 2.9337179922054744, acc: 0.5694833333333333\n",
      "iter[44, loss: 2.9020454717668724, acc: 0.5737666666666666\n",
      "iter[45, loss: 2.8715868542996317, acc: 0.5784666666666667\n",
      "iter[46, loss: 2.8422647201228086, acc: 0.5831333333333333\n",
      "iter[47, loss: 2.814017086902312, acc: 0.5874166666666667\n",
      "iter[48, loss: 2.7867808800691205, acc: 0.5913166666666667\n",
      "iter[49, loss: 2.760500131877618, acc: 0.5959\n",
      "iter[50, loss: 2.7351252631406173, acc: 0.5996666666666667\n",
      "iter[51, loss: 2.7106054544570015, acc: 0.6034833333333334\n",
      "iter[52, loss: 2.6868966022481793, acc: 0.6068666666666667\n",
      "iter[53, loss: 2.6639456327605564, acc: 0.61055\n",
      "iter[54, loss: 2.6417327435892988, acc: 0.6140666666666666\n",
      "iter[55, loss: 2.6202201878188607, acc: 0.6175833333333334\n",
      "iter[56, loss: 2.599373084748381, acc: 0.6211\n",
      "iter[57, loss: 2.579165126558679, acc: 0.6243333333333333\n",
      "iter[58, loss: 2.559573576830925, acc: 0.6274666666666666\n",
      "iter[59, loss: 2.5405632625049503, acc: 0.6306333333333334\n",
      "iter[60, loss: 2.5221048205716765, acc: 0.6338666666666667\n",
      "iter[61, loss: 2.504176585482343, acc: 0.6370666666666667\n",
      "iter[62, loss: 2.486752359779679, acc: 0.6404333333333333\n",
      "iter[63, loss: 2.469816506288329, acc: 0.6436333333333333\n",
      "iter[64, loss: 2.4533500525899194, acc: 0.6463833333333333\n",
      "iter[65, loss: 2.437334044711108, acc: 0.6488333333333334\n",
      "iter[66, loss: 2.4217543346328325, acc: 0.6515166666666666\n",
      "iter[67, loss: 2.406588945344073, acc: 0.6545833333333333\n",
      "iter[68, loss: 2.3918151371928134, acc: 0.6570166666666667\n",
      "iter[69, loss: 2.3774258926855647, acc: 0.6594333333333333\n",
      "iter[70, loss: 2.363402306590286, acc: 0.6619833333333334\n",
      "iter[71, loss: 2.3497314637722067, acc: 0.6643833333333333\n",
      "iter[72, loss: 2.3363989351806698, acc: 0.6669333333333334\n",
      "iter[73, loss: 2.323389975558047, acc: 0.6690833333333334\n",
      "iter[74, loss: 2.310692195891725, acc: 0.6715166666666667\n",
      "iter[75, loss: 2.2982951493454964, acc: 0.6737833333333333\n",
      "iter[76, loss: 2.2861951420213398, acc: 0.6761166666666667\n",
      "iter[77, loss: 2.274381025787602, acc: 0.67835\n",
      "iter[78, loss: 2.2628409609716593, acc: 0.6802166666666667\n",
      "iter[79, loss: 2.2515712111287582, acc: 0.6819333333333333\n",
      "iter[80, loss: 2.2405595758265076, acc: 0.6839\n",
      "iter[81, loss: 2.229795182554253, acc: 0.6858833333333333\n",
      "iter[82, loss: 2.219268720263572, acc: 0.6876\n",
      "iter[83, loss: 2.2089783563647245, acc: 0.6892333333333334\n",
      "iter[84, loss: 2.1989111878802294, acc: 0.6906333333333333\n",
      "iter[85, loss: 2.189058718568376, acc: 0.6927666666666666\n",
      "iter[86, loss: 2.1794104044920735, acc: 0.6945333333333333\n",
      "iter[87, loss: 2.1699668897043107, acc: 0.6961333333333334\n",
      "iter[88, loss: 2.16072294314142, acc: 0.6979666666666666\n",
      "iter[89, loss: 2.151671448619098, acc: 0.69925\n",
      "iter[90, loss: 2.1428023890718735, acc: 0.7011166666666667\n",
      "iter[91, loss: 2.134114761212375, acc: 0.7028666666666666\n",
      "iter[92, loss: 2.1256034119369858, acc: 0.7044\n",
      "iter[93, loss: 2.117261008092441, acc: 0.70625\n",
      "iter[94, loss: 2.1090855385200244, acc: 0.7079166666666666\n",
      "iter[95, loss: 2.101071762996144, acc: 0.7094833333333334\n",
      "iter[96, loss: 2.093216878040206, acc: 0.7107666666666667\n",
      "iter[97, loss: 2.0855131123415998, acc: 0.712\n",
      "iter[98, loss: 2.0779572284284087, acc: 0.7133833333333334\n",
      "iter[99, loss: 2.0705424352428152, acc: 0.71455\n",
      "iter[100, loss: 2.0632674454877167, acc: 0.7161\n",
      "iter[101, loss: 2.056127748533415, acc: 0.7175166666666667\n",
      "iter[102, loss: 2.049121223372426, acc: 0.7187666666666667\n",
      "iter[103, loss: 2.0422456254944494, acc: 0.7202833333333334\n",
      "iter[104, loss: 2.03549646644874, acc: 0.72175\n",
      "iter[105, loss: 2.0288700336574066, acc: 0.7230166666666666\n",
      "iter[106, loss: 2.0223643035148937, acc: 0.72415\n",
      "iter[107, loss: 2.0159758216179875, acc: 0.72525\n",
      "iter[108, loss: 2.0097013811923605, acc: 0.7261166666666666\n",
      "iter[109, loss: 2.003539772140856, acc: 0.7274666666666667\n",
      "iter[110, loss: 1.9974860549560267, acc: 0.7283666666666667\n",
      "iter[111, loss: 1.9915408336315312, acc: 0.7296\n",
      "iter[112, loss: 1.9857002616955324, acc: 0.7311166666666666\n",
      "iter[113, loss: 1.979960171999377, acc: 0.7322666666666666\n",
      "iter[114, loss: 1.9743160481573019, acc: 0.73305\n",
      "iter[115, loss: 1.9687687829511284, acc: 0.7341333333333333\n",
      "iter[116, loss: 1.963317622818819, acc: 0.7350833333333333\n",
      "iter[117, loss: 1.9579571750432105, acc: 0.7362833333333333\n",
      "iter[118, loss: 1.9526836954964666, acc: 0.7374166666666667\n",
      "iter[119, loss: 1.9474988855069955, acc: 0.7383333333333333\n",
      "iter[120, loss: 1.9423999490789985, acc: 0.73935\n",
      "iter[121, loss: 1.9373864593403998, acc: 0.7401833333333333\n",
      "iter[122, loss: 1.9324547721109073, acc: 0.7409833333333333\n",
      "iter[123, loss: 1.927604512172377, acc: 0.7419666666666667\n",
      "iter[124, loss: 1.9228305109447152, acc: 0.74285\n",
      "iter[125, loss: 1.9181289359172267, acc: 0.7437\n",
      "iter[126, loss: 1.9135020027780425, acc: 0.7446666666666667\n",
      "iter[127, loss: 1.9089517894598322, acc: 0.74565\n",
      "iter[128, loss: 1.9044759560286708, acc: 0.7462166666666666\n",
      "iter[129, loss: 1.9000733561836527, acc: 0.7469666666666667\n",
      "iter[130, loss: 1.895741592766489, acc: 0.7477666666666667\n",
      "iter[131, loss: 1.8914780574135241, acc: 0.7487333333333334\n",
      "iter[132, loss: 1.8872825505708495, acc: 0.74945\n",
      "iter[133, loss: 1.8831530540743961, acc: 0.7503333333333333\n",
      "iter[134, loss: 1.8790875755283072, acc: 0.7511666666666666\n",
      "iter[135, loss: 1.8750871047128936, acc: 0.7519\n",
      "iter[136, loss: 1.8711488780410916, acc: 0.7525833333333334\n",
      "iter[137, loss: 1.8672685159302993, acc: 0.7536333333333334\n",
      "iter[138, loss: 1.8634446207726623, acc: 0.7546333333333334\n",
      "iter[139, loss: 1.859676331059339, acc: 0.7555\n",
      "iter[140, loss: 1.8559640285164036, acc: 0.75615\n",
      "iter[141, loss: 1.852309071014541, acc: 0.7567333333333334\n",
      "iter[142, loss: 1.8487104161055974, acc: 0.7574666666666666\n",
      "iter[143, loss: 1.8451673534590463, acc: 0.7582333333333333\n",
      "iter[144, loss: 1.8416779963792778, acc: 0.7592666666666666\n",
      "iter[145, loss: 1.8382398354778657, acc: 0.75985\n",
      "iter[146, loss: 1.834850273455972, acc: 0.7604166666666666\n",
      "iter[147, loss: 1.831511898875262, acc: 0.7610166666666667\n",
      "iter[148, loss: 1.8282232327467292, acc: 0.7619\n",
      "iter[149, loss: 1.8249842066866622, acc: 0.7626666666666667\n",
      "iter[150, loss: 1.8217939966687031, acc: 0.7634666666666666\n",
      "iter[151, loss: 1.8186497058982176, acc: 0.7640166666666667\n",
      "iter[152, loss: 1.8155464453888714, acc: 0.7646\n",
      "iter[153, loss: 1.8124863586662903, acc: 0.7652166666666667\n",
      "iter[154, loss: 1.8094696534763364, acc: 0.7658\n",
      "iter[155, loss: 1.8064967036079345, acc: 0.7664833333333333\n",
      "iter[156, loss: 1.8035642799597016, acc: 0.76705\n",
      "iter[157, loss: 1.8006726365577967, acc: 0.7676666666666667\n",
      "iter[158, loss: 1.7978210556826424, acc: 0.7682833333333333\n",
      "iter[159, loss: 1.795010526668569, acc: 0.7689666666666667\n",
      "iter[160, loss: 1.7922391941762008, acc: 0.7696333333333333\n",
      "iter[161, loss: 1.7895073903401635, acc: 0.7700666666666667\n",
      "iter[162, loss: 1.7868118701919904, acc: 0.7705666666666666\n",
      "iter[163, loss: 1.784152132464323, acc: 0.7711833333333333\n",
      "iter[164, loss: 1.7815282888264568, acc: 0.7716666666666666\n",
      "iter[165, loss: 1.7789395820479958, acc: 0.7723166666666667\n",
      "iter[166, loss: 1.77638685270106, acc: 0.7727833333333334\n",
      "iter[167, loss: 1.773869592621541, acc: 0.7735666666666666\n",
      "iter[168, loss: 1.771386432078867, acc: 0.7740666666666667\n",
      "iter[169, loss: 1.7689370426055784, acc: 0.7745666666666666\n",
      "iter[170, loss: 1.766519089601084, acc: 0.7751333333333333\n",
      "iter[171, loss: 1.7641314552233411, acc: 0.7756666666666666\n",
      "iter[172, loss: 1.761776809012896, acc: 0.7764166666666666\n",
      "iter[173, loss: 1.7594543377610576, acc: 0.77705\n",
      "iter[174, loss: 1.7571642414163522, acc: 0.7776333333333333\n",
      "iter[175, loss: 1.7549036394718014, acc: 0.7779833333333334\n",
      "iter[176, loss: 1.7526727589412916, acc: 0.7786333333333333\n",
      "iter[177, loss: 1.750471615615569, acc: 0.7790833333333333\n",
      "iter[178, loss: 1.7482978170428842, acc: 0.7795166666666666\n",
      "iter[179, loss: 1.746151695548013, acc: 0.77995\n",
      "iter[180, loss: 1.7440341004823596, acc: 0.7804\n",
      "iter[181, loss: 1.7419448794024592, acc: 0.7809333333333334\n",
      "iter[182, loss: 1.7398815840523967, acc: 0.7816833333333333\n",
      "iter[183, loss: 1.7378437858464395, acc: 0.7822\n",
      "iter[184, loss: 1.735830730580083, acc: 0.7827166666666666\n",
      "iter[185, loss: 1.7338441733608565, acc: 0.7831666666666667\n",
      "iter[186, loss: 1.7318843987180124, acc: 0.7834666666666666\n",
      "iter[187, loss: 1.7299468954273394, acc: 0.7839166666666667\n",
      "iter[188, loss: 1.7280335021021753, acc: 0.78445\n",
      "iter[189, loss: 1.7261452348055857, acc: 0.7849666666666667\n",
      "iter[190, loss: 1.7242817161389514, acc: 0.7853666666666667\n",
      "iter[191, loss: 1.7224414255081422, acc: 0.7858\n",
      "iter[192, loss: 1.7206246641694356, acc: 0.7861833333333333\n",
      "iter[193, loss: 1.7188314653247256, acc: 0.7865666666666666\n",
      "iter[194, loss: 1.717061903871822, acc: 0.7869333333333334\n",
      "iter[195, loss: 1.715315593355649, acc: 0.7872666666666667\n",
      "iter[196, loss: 1.713590972830627, acc: 0.78765\n",
      "iter[197, loss: 1.711887771775046, acc: 0.7879833333333334\n",
      "iter[198, loss: 1.7102060065247349, acc: 0.7883333333333333\n",
      "iter[199, loss: 1.7085450414132806, acc: 0.7887666666666666\n"
     ]
    }
   ],
   "source": [
    "# 开始训练\n",
    "\n",
    "w = np.random.randn(n_class, n_featrues)\n",
    "b = np.random.randn(1, n_class)\n",
    "\n",
    "eta = 1e-5  # 学习率\n",
    "for i in range(200):\n",
    "    y_hat = softmax_regression(x, w, b)\n",
    "    # 直接计算梯度\n",
    "    grad_w, grad_b = gradients(y_hat, y, x)\n",
    "    # 更新参数\n",
    "    w = w - eta * grad_w\n",
    "    b = b - eta * grad_b\n",
    "    loss = cross_entropy(y_hat, y)\n",
    "    print(f'iter[{i}, loss: {loss}, acc: {accuracy(y_hat, y)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "240684a6bf55e82a0f5995d45026058ae310044e71522ea1b595ad868521a9f2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
