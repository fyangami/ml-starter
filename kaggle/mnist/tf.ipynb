{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mnist import train_images, train_labels, test_images, test_labels\n",
    "\n",
    "n_feature = 28 * 28\n",
    "n_class = 10\n",
    "\n",
    "# train_data = train_images().reshape(-1, n_feature).astype('float64') / 255.\n",
    "# train_labels = train_labels()\n",
    "# test_data = test_images().reshape(-1, n_feature).astype('float64') / 255.\n",
    "# test_labels = test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用tensorflow做逻辑回归\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(train_data, train_labels), (test_data, test_labels) = mnist.load_data()\n",
    "# 数据集拆分\n",
    "train_data, test_data = np.array(train_data, np.float32), np.array(test_data, np.float32)\n",
    "# Flatten images to 1-D vector of 784 features (28*28).\n",
    "train_data, test_data = train_data.reshape([-1, n_feature]), test_data.reshape([-1, n_feature])\n",
    "# Normalize images value from [0, 255] to [0, 1].\n",
    "train_data, test_data = train_data / 255., test_data / 255.\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n",
    "train_data = train_data.repeat().shuffle(5000).batch(1000).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def softmax(x):\n",
    "    exp = tf.exp(x)\n",
    "    return exp / tf.reduce_sum(exp, 1, keepdims=True)\n",
    "\n",
    "def net(x, w, b):\n",
    "    return softmax(tf.matmul(x, w) + b)\n",
    "\n",
    "# Logistic regression (Wx + b).\n",
    "# def net(x, w, b):\n",
    "#     # Apply softmax to normalize the logits to a probability distribution.\n",
    "#     return tf.nn.softmax(tf.matmul(x, w) + b)\n",
    "\n",
    "# def cross_entropy(y_pred, y_true):\n",
    "#     # Encode label to a one hot vector.\n",
    "#     y_true = tf.one_hot(y_true, depth=n_class)\n",
    "#     # Clip prediction values to avoid log(0) error.\n",
    "#     y_pred = tf.clip_by_value(y_pred, 1e-9, 1.)\n",
    "#     # Compute cross-entropy.\n",
    "#     return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred),1))\n",
    "def cross_entropy(y_hat, y):\n",
    "    y_hat = tf.clip_by_value(y_hat, 1e-9, 1.)\n",
    "    return -tf.math.log(tf.boolean_mask(y_hat, tf.one_hot(y, depth=y_hat.shape[-1])))\n",
    "\n",
    "# Accuracy metric.\n",
    "# def accuracy(y_pred, y_true):\n",
    "#     # Predicted class is the index of highest score in prediction vector (i.e. argmax).\n",
    "#     correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
    "#     return tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "def accuracy(y_hat, y):\n",
    "    y_hat = tf.argmax(y_hat, axis=1)\n",
    "    cmp = tf.cast(y_hat, y.dtype) == y\n",
    "    return tf.reduce_sum(tf.cast(cmp, y.dtype)) / len(y)\n",
    "\n",
    "eta = 1e-2\n",
    "w = tf.Variable(tf.random.normal((n_feature, n_class)))\n",
    "b = tf.Variable(tf.zeros((n_class)))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(eta)\n",
    "losses = list()\n",
    "for i, (batch_x, batch_y) in enumerate(train_data.take(4000)):\n",
    "    with tf.GradientTape() as g:\n",
    "        y_hat = net(batch_x, w,  b)\n",
    "        loss = cross_entropy(y_hat, batch_y)\n",
    "    grads = g.gradient(loss, [w, b])\n",
    "    optimizer.apply_gradients(zip(grads, [w, b]))\n",
    "    ty_hat = net(test_data, w, b)\n",
    "    acc = accuracy(ty_hat, test_labels)\n",
    "    losses.append(loss.numpy().mean())\n",
    "    if i % 200 == 0:\n",
    "        print(f'loss: {loss.numpy().mean()}, test acc: {acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.argmax(net(test_data[:10], w, b)))\n",
    "print(test_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "240684a6bf55e82a0f5995d45026058ae310044e71522ea1b595ad868521a9f2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
