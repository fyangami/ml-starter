{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, tokens) -> None:\n",
    "        if isinstance(tokens[0], list):\n",
    "            tokens = [token for line in tokens for token in line]\n",
    "        import collections\n",
    "        counter = collections.Counter(tokens)\n",
    "        tokens = [\n",
    "            k for k, _ in sorted(\n",
    "                counter.items(), key=lambda item: item[1], reverse=True)\n",
    "        ]\n",
    "        tokens.insert(0, '<unk>')\n",
    "        self.tokens_indicates = {\n",
    "            token: idx\n",
    "            for idx, token in enumerate(tokens)\n",
    "        }\n",
    "        self.indicates_tokens = {\n",
    "            v: k\n",
    "            for k, v in self.tokens_indicates.items()\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_indicates)\n",
    "\n",
    "    def __getitem__(self, keys):\n",
    "        if isinstance(keys, str):\n",
    "            return self.tokens_indicates[keys]\n",
    "        if isinstance(keys, list):\n",
    "            return [self.__getitem__(key) for key in keys]\n",
    "        if isinstance(keys, (torch.Tensor)):\n",
    "            keys = keys.reshape(-1)\n",
    "            return ''.join(self.indicates_tokens[int(keys[i])]\n",
    "                           for i in range(keys.numel()))\n",
    "        return self.indicates_tokens[keys]\n",
    "\n",
    "truncate = lambda sen, l: sen[:l] if len(sen) > l else sen + ['<pad>'] * (\n",
    "    l - len(sen))\n",
    "\n",
    "def tokenize_zh(sentence, steps):\n",
    "    import jieba\n",
    "    import zhconv\n",
    "    sentence_zh = list(\n",
    "            jieba.cut(zhconv.convert(sentence, 'zh-cn'), cut_all=False))\n",
    "\n",
    "    sentence_zh = ['<bos>'] + sentence_zh + ['<eos>']\n",
    "    sentence_zh = truncate(sentence_zh, steps)\n",
    "    return sentence_zh\n",
    "\n",
    "def tokenize_en(sentence, steps):\n",
    "    import re\n",
    "    sentence_en = [\n",
    "        i for i in re.sub('[^A-Za-z ]+', lambda m: f' {m.group()} ',\n",
    "                        sentence).lower().split(' ')\n",
    "        if i != '' and i != ' '\n",
    "    ]\n",
    "    sentence_en = sentence_en + ['<eos>']\n",
    "    sentence_en = truncate(sentence_en, steps)\n",
    "    return sentence_en\n",
    "\n",
    "def tokenize(lines: list, steps=32):\n",
    "    en, zh = [], []\n",
    "    for line in lines:\n",
    "        sentence_en, sentence_zh, _ = line.split('\\t')\n",
    "        en.append(tokenize_en(sentence_en, steps))\n",
    "        zh.append(tokenize_zh(sentence_zh, steps))\n",
    "    return en, zh\n",
    "\n",
    "\n",
    "def read_data():\n",
    "    with open('../rnn/en_zh.trans.txt', 'r') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "\n",
    "class _Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_raw) -> None:\n",
    "        super().__init__()\n",
    "        en, zh = data_raw\n",
    "        self.vocab_en, self.vocab_zh = Vocab(en), Vocab(zh)\n",
    "        self.corpus_en, self.corpus_zh = torch.tensor(\n",
    "            self.vocab_en[en],\n",
    "            dtype=torch.int64), torch.tensor(self.vocab_zh[zh],\n",
    "                                               dtype=torch.int64)\n",
    "        self.valid_len_en, self.valid_len_zh = (\n",
    "            self.corpus_en != self.vocab_en['<pad>']).sum(\n",
    "                dim=1), (self.corpus_zh != self.vocab_zh['<pad>']).sum(dim=1)\n",
    "        self._len = len(self.corpus_en)\n",
    "        del en, zh\n",
    "        assert self._len == len(self.corpus_zh) == len(\n",
    "            self.valid_len_en) == len(self.valid_len_zh)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.corpus_en[idx], self.valid_len_en[idx], self.corpus_zh[\n",
    "            idx], self.valid_len_zh[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_size,\n",
    "                 n_hiddens,\n",
    "                 n_layers,\n",
    "                 dropout=0.) -> None:\n",
    "        super().__init__()\n",
    "        # embedding layer similar as `one-hot` transformation.\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, n_hiddens, n_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x.shape = (batch_size, n_steps)\n",
    "        # embed.shape = (n_steps, batch_size, embed_size)\n",
    "        embed = self.embedding(x.T.type(torch.int64))\n",
    "        # output.shape = (n_steps, batch_size, n_hiddens)\n",
    "        # state.shape = (n_layers, batch_size, n_hiddens)\n",
    "        output, state = self.rnn(embed)\n",
    "        return output, state\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 embed_size,\n",
    "                 n_hiddens,\n",
    "                 n_layers,\n",
    "                 dropout=0.) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + n_hiddens,\n",
    "                          n_hiddens,\n",
    "                          n_layers,\n",
    "                          dropout=dropout)\n",
    "        self.fc = nn.Linear(n_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, encode_outputs):\n",
    "        encode_output, encode_state = encode_outputs\n",
    "        return encode_state\n",
    "\n",
    "    def forward(self, x, es: torch.Tensor):\n",
    "        embed = self.embedding(x.T.type(torch.int64))\n",
    "        # context using last layer of encode state\n",
    "        # c.shape = (batch_size, n_hiddens)\n",
    "        c = es[-1]\n",
    "        c = c.repeat(embed.shape[0], 1, 1)\n",
    "        embed_c = torch.cat((embed, c), -1)\n",
    "        # decoder initial state as final `output_state` of encoder\n",
    "        output, state = self.rnn(embed_c, es)\n",
    "        output = self.fc(output)\n",
    "        # output.shape = (batch_size, n_steps, vocab_size)\n",
    "        return output, state\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size_source,\n",
    "                 vocab_size_target,\n",
    "                 embed_size,\n",
    "                 n_hiddens,\n",
    "                 n_layers,\n",
    "                 dropout=0.) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size=vocab_size_source,\n",
    "                               embed_size=embed_size,\n",
    "                               n_hiddens=n_hiddens,\n",
    "                               n_layers=n_layers,\n",
    "                               dropout=dropout)\n",
    "        self.decoder = Decoder(vocab_size=vocab_size_target,\n",
    "                               embed_size=embed_size,\n",
    "                               n_hiddens=n_hiddens,\n",
    "                               n_layers=n_layers,\n",
    "                               dropout=dropout)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        output = self.encoder(x)\n",
    "        decode_state = self.decoder.init_state(output)\n",
    "        output, _ = self.decoder(y, decode_state)\n",
    "        return output\n",
    "\n",
    "def loss_masking(loss, y, pad_indicate):\n",
    "    mask = (y != pad_indicate).type(torch.float32)\n",
    "    return (loss * mask).sum() / mask.sum()\n",
    "\n",
    "def grad_clip(net: nn.Module, clip_val=1):\n",
    "    params = [p for p in net.parameters() if p.requires_grad]\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > clip_val:\n",
    "        for parm in params:\n",
    "            parm.grad[:] *= clip_val / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "EPOCHS = 50\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 128\n",
    "STEPS = 19\n",
    "EMBED_SIZE = 256\n",
    "HIDDENS = 256\n",
    "LAYERS = 3\n",
    "DROPOUT = .3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/fyang/code/ml/ml-starter/nlp/trans.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fyang/code/ml/ml-starter/nlp/trans.ipynb#ch0000004?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fyang/code/ml/ml-starter/nlp/trans.ipynb#ch0000004?line=18'>19</a>\u001b[0m \u001b[39m# 1024 32 28\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/fyang/code/ml/ml-starter/nlp/trans.ipynb#ch0000004?line=19'>20</a>\u001b[0m output \u001b[39m=\u001b[39m net(x, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fyang/code/ml/ml-starter/nlp/trans.ipynb#ch0000004?line=20'>21</a>\u001b[0m \u001b[39m# permute axises to (batch_size, vocab_size, n_steps)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fyang/code/ml/ml-starter/nlp/trans.ipynb#ch0000004?line=21'>22</a>\u001b[0m \u001b[39m# loss_fn required input.shape=(B, C, ...) where C = vocab_size\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fyang/code/ml/ml-starter/nlp/trans.ipynb#ch0000004?line=22'>23</a>\u001b[0m \u001b[39m# see also `torch.nn.CrossEntropyLoss` for detail.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fyang/code/ml/ml-starter/nlp/trans.ipynb#ch0000004?line=23'>24</a>\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mpermute((\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/mlenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/fyang/code/ml/ml-starter/nlp/trans.ipynb Cell 5\u001b[0m in \u001b[0;36mEncoderDecoder.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fyang/code/ml/ml-starter/nlp/trans.ipynb#ch0000004?line=76'>77</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fyang/code/ml/ml-starter/nlp/trans.ipynb#ch0000004?line=77'>78</a>\u001b[0m decode_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39minit_state(output)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/fyang/code/ml/ml-starter/nlp/trans.ipynb#ch0000004?line=78'>79</a>\u001b[0m output, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(y, decode_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fyang/code/ml/ml-starter/nlp/trans.ipynb#ch0000004?line=79'>80</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.conda/envs/mlenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/fyang/code/ml/ml-starter/nlp/trans.ipynb Cell 5\u001b[0m in \u001b[0;36mDecoder.forward\u001b[0;34m(self, x, es)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fyang/code/ml/ml-starter/nlp/trans.ipynb#ch0000004?line=48'>49</a>\u001b[0m \u001b[39m# decoder initial state as final `output_state` of encoder\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fyang/code/ml/ml-starter/nlp/trans.ipynb#ch0000004?line=49'>50</a>\u001b[0m output, state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(embed_c, es)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/fyang/code/ml/ml-starter/nlp/trans.ipynb#ch0000004?line=50'>51</a>\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(output)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fyang/code/ml/ml-starter/nlp/trans.ipynb#ch0000004?line=51'>52</a>\u001b[0m \u001b[39m# output.shape = (batch_size, n_steps, vocab_size)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/fyang/code/ml/ml-starter/nlp/trans.ipynb#ch0000004?line=52'>53</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output, state\n",
      "File \u001b[0;32m~/.conda/envs/mlenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/mlenv/lib/python3.10/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = _Dataset(tokenize(read_data(), steps=STEPS))\n",
    "dataloader = torch.utils.data.DataLoader(dataset,\n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         shuffle=True)\n",
    "net = EncoderDecoder(vocab_size_source=len(dataset.vocab_en),\n",
    "                     vocab_size_target=len(dataset.vocab_zh),\n",
    "                     embed_size=EMBED_SIZE,\n",
    "                     n_hiddens=HIDDENS,\n",
    "                     n_layers=LAYERS,\n",
    "                     dropout=DROPOUT)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "losses, loss = [], None\n",
    "net.zero_grad()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_iter = iter(dataloader)\n",
    "    for x, _, y, _ in train_iter:\n",
    "        optimizer.zero_grad()\n",
    "        # 1024 32 28\n",
    "        output = net(x, y)\n",
    "        # permute axises to (batch_size, vocab_size, n_steps)\n",
    "        # loss_fn required input.shape=(B, C, ...) where C = vocab_size\n",
    "        # see also `torch.nn.CrossEntropyLoss` for detail.\n",
    "        output = output.permute((1, 2, 0))\n",
    "        loss = loss_fn(output, y)\n",
    "        loss = loss_masking(loss, y, dataset.vocab_zh['<pad>'])\n",
    "        with torch.no_grad():\n",
    "            loss.backward()\n",
    "            grad_clip(net)\n",
    "            optimizer.step()\n",
    "    print(f\"\\repoch: [{epoch}/{EPOCHS}\", end='')\n",
    "    losses.append(loss.detach().numpy())\n",
    "    # print(f'loss: {loss: .6f}')\n",
    "print()\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    softmax = nn.LogSoftmax(dim=1)\n",
    "    token = tokenize_en(sentence, steps=STEPS)\n",
    "    x = torch.tensor(dataset.vocab_en[token], dtype=torch.int64)\n",
    "    x = x.unsqueeze(0)\n",
    "    output = net.encoder(x)\n",
    "    state = net.decoder.init_state(output)\n",
    "    y = dataset.vocab_zh['<bos>']\n",
    "    y = torch.tensor([y], dtype=torch.int64)\n",
    "    # add batch_axis\n",
    "    y = y.unsqueeze(0)\n",
    "    outputs = []\n",
    "    for _ in range(STEPS):\n",
    "        output, _ = net.decoder(y, state)\n",
    "        \n",
    "        outputs.append(output.argmax(dim=2))\n",
    "        y = outputs[-1]\n",
    "    return ''.join(dataset.vocab_zh[outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos><bos>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"hello\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ae33f7c48cc3e1271596d1bf08ce4d5e6d6f7129ff8bbb83bbb95ed8addff62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
